---
title: "Remote Sensing Data"
format: html
---

## **Summary**

This week introduced a lot of new concepts related to corrections, data joining, and enhancements in remote sensing. To keep things clear for future reference, I organized them into categories and decided to do kind of a "dictionary" of these concepts, so it will be useful to understand them when reviewing literature and seeing applications of them.

*Image 1. Quick summary of main concepts mentioned in class this week*

![](images/clipboard-3007794880.png){width="493"}

### Key terms

So far many words have been repeated multiple times during lectures and papers I have reviewed and sometimes I still mix them up, so here they are summarised:

| Term | Meaning |
|------------------|------------------------------------------------------|
| Irradiance | Radiation reaching Earth from the Sun |
| Radiance | Radiation leaving Earth toward the satellite |
| Reflectance | The proportion of incoming radiation reflected by a surface |
| Surface Reflectance | Reflectance at the bottom of the atmosphere (after atmospheric correction) |
| Analysis-ready data | Preprocessed images, usually in Surface Reflectance format (so we don’t have to do all the corrections ourselves!) |

*Image 2. Illustration done to summarise the main terms used in Remote Sensing*

![](images/clipboard-2014150389.jpeg){fig-align="center" width="397"}

### Corrections:

This week we saw some of the corrections that raw sensor data could use before we start manipulating it. Most of them are already applied to images we use, as we usually use Analysis-ready data, but I think knowing what kind of preprocessing images have is useful at least as a general knowledge or if we were to work on raw data(maybe needed if I were to work with drone imagery?).

| Term | Meaning |
|----------------------|--------------------------------------------------------------------|
| Geometric correction | Aligns misaligned images using control points and regression (basically shifting everything so it matches properly). Useful for correcting historical maps to match modern spatial data. |
| Atmospheric correction | Removes distortions caused by atmospheric scattering (because the atmosphere its always there!) |
| Dark Object Subtraction (DOS) | Method for atmospheric correction that finds something that should have zero reflectance and subtracts its measured value from all pixels |
| Pseudo-Invariant Features (PIFs) | Method for atmospheric correction that uses objects with stable reflectance over time as references for correction (great for long-term studies) |
| Flat correction | Method for atmospheric correction that uses field-measured values (without atmospheric interference) as a baseline for regression |
| Orthorectification | Correction that fixes distortions from sensor tilt |
| Radiometric calibration | Converts Digital Number (DN) to spectral radiance to standardize measurements |

### Enhancements:

I would say enhancements are a way of transforming images using data that is already available in the image itself, to better show some features. Some of the methods are:

| Term | Meaning |
|-----------------------|-------------------------------------------------------------------|
| Contrast adjustments | Modifies image appearance without changing data values (basically, just making it look better) |
| Band ratios | Dividing one band by another (e.g., NDVI for vegetation analysis) |
| Filtering | Can be low-pass (smoothing) or high-pass (highlighting edges, like for detecting buildings) |
| Principal Component Analysis (PCA) | Reduces dimensionality to capture the most variance (helps focus on key changes in images, but could loose some interpretability) |
| Texture analysis | Measures similarity between a pixel and its neighbors (useful for spotting urban areas or specific land features) |
| Image fusion & pansharpening | Merges data from different sensors, often using high-resolution bands to sharpen lower-resolution images |

## **Applications**

For this applications section, I decided to look into two ways of enhancing remote sensing data to understand better how to leverage them. One using multisensor fusion (pansharpening) and the other using textural images to enhance land-cover classification. Both methods aimed to get more useful information out of satellite images, but they do it in very different ways.

### Multisensor fusion: pansharpening example

In this research by [Siok et al.(2020)](https://www.mdpi.com/1424-8220/20/24/7100), pansharpening was used to enhance the spatial resolution of multispectral images by combining them with higher-resolution panchromatic images. The goal was to improve the clarity and detail of the images while preserving their spectral information. The study compared different pansharpening techniques to assess their effectiveness in maintaining color accuracy and spatial sharpness. Their results showed that certain methods performed better than others, depending on the specific application and the type of satellite data used and also that the effectiveness varied by land cover type, where forests had the least improvement.

*Image 3. On the left samples of the best high-spatial-images. On the right, samples of images in natural color composition after being enhanced with the best high-spatial-resolution image*

![](images/clipboard-4145642814.png){fig-align="center" width="370"}

### Textural images for improving land-cover classification in the Brazilian Amazon

This study by [Lu et al. (2014)](https://www.tandfonline.com/doi/full/10.1080/01431161.2014.980920#d1e206) explores how textural images from different satellite sensors can improve land-cover classification, particularly in the Brazilian Amazon. The researchers analyzed images from different sensor which have different spatial resolutions, to determine how texture affects classification accuracy. They used a GLCM texture model, and found that adding textural images to radiometric data can enhance classification, especially for high-resolution sensors. Some of their key findings were that not all textures are equally useful and that the best results come from combining two textural images rather than using many, and the ideal texture combinations vary by sensor. Overall, they found that while spectral data remains more effective than texture for distinguishing land-cover types, texture significantly improves classification, especially at higher resolutions.

*Image 4. Textural images obtained from the SPOT panchromatic band based on different texture measures but the same window size (9 × 9) for Machadinho d’Oeste; (a), (b), (c), (d), (e), (f), (g), and (h) represent the textural images calculated using mean, variance, homogeneity, contrast, dissimilarity, entropy, second moment, and correlation coefficient, respectively.*

![](images/clipboard-1445595291.png){fig-align="center" width="370"}

### **Some thoughts:**

Both studies showed that enhancing remote sensing images, whether by improving spatial resolution or adding texture, can really boost their usefulness. But what really stood out to me is that there’s no “one-size-fits-all” method. Some pansharpening techniques work better than others depending on the land cover, and not all textures are useful for classification or depend on what you are trying to classify or what type of remote sensing data you use. Also, it seems like higher resolution always helps, but only up to a point (so when does more detail stop being helpful?). Overall, it’s cool to see how much thought goes into picking the right enhancement techniques as I would have originally thought that it was something more standard.

## **Reflections**

At first, learning about corrections felt unnecessary as most modern datasets come preprocessed anyway. But after the lecture I think it's worth knowing the possibilities we have and what we could do if we ever encounter raw data(maybe not even satellite data). Even if we end up using only Analysis-ready data, I now think it's important to know what kind of process it has been through so we can assess it better and know what we are working with. Enhancements, instead, felt like something I could be using more in my analysis, and I confirmed it specially after seeing the applications. I found them to be a great toolset to get more information from our images, but also something we need to choose case by case. Now, despite how important these concepts are, I still found this week to be a bit overwhelming because of all the concepts, but after organizing everything, it started making more sense. Some papers are still hard to follow with all their complex formulas, but at least now I have a better foundation to understand what’s going on, so I guess we are having progress!

On another topics, a couple things caught my attention this week. One thing that surprised me was how much regression is used in remote sensing. I always thought of regression as something for statistical modeling (like in CASA007), but it's actually everywhere here, aligning images, calibrating radiance, enhancing quality, etc. The other one was seeing Andy's fieldwork for atmospheric correction which seems pretty cool. It made me think that sometimes we might need higher levels of precision for our images and its interesting to see all the options we have available to handle that.
