[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CASA0023 Learning Diary",
    "section": "",
    "text": "Introduction\nHello world!\nThis Learning Diary is part of the CASA0023 module, Remotely Sensing Cities and Environments, which I’m taking as part of the MSc Urban Spatial Science at UCL. Here, you’ll find weekly entries summarizing key takeaways from each lecture, examples of practical applications, and my personal reflections.\nBut first, let me introduce myself!\n\n\nMy name is Fiorella, and I’m currently pursuing an MSc in Urban Spatial Science. Back in 2021, I graduated as an architect and urbanist in Peru. Since then, I have developed a strong interest in sustainability, climate change, water resources, and urban settlements, particularly how these factors affect urban and natural environments. As an example, during my undergraduate thesis, I focused on assessing changes in a river in the Peruvian Amazon, accelerated by climate change, and proposed solutions to the urban challenges that emerged from these changes.\n\n\n\n\n\nSince graduating, I have worked in the public sector, contributing to Lima’s Rimac River renaturalization project. This role involved urban analysis, master planning, urban design, and policy implementation, with a strong emphasis on risk prevention, and nature-based solutions.\nWhile I have some basic experience using Earth Observation (EO) and remote sensing to address topics like those mentioned above, my approach has been largely empirical. That’s why I find this module to be an exciting opportunity to deepen my knowledge of these tools and processes, enabling me to better inform policies and urban solutions in future projects or research.\nI am particularly interested in how these technologies can be applied to urban challenges in the Global South, where they are often underutilized due to knowledge gaps. That’s why, throughout this diary, my reflections will aim to connect these tools to my background and interests, as well as explore their real-world applications in contexts similar to my country’s.\nAnd if you are curious about my previous projects, here you can have a glimpse at them!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Welcome</span>"
    ]
  },
  {
    "objectID": "W1_Introduction.html",
    "href": "W1_Introduction.html",
    "title": "2  An Introduction to Remote Sensing",
    "section": "",
    "text": "2.1 Summary\nThis week’s lecture covered the basics of Earth Observation (EO) and Remote Sensing including topics such as definitions, relevance and terminology of Earth Observation, as well as more technical concepts to describe sensors (resolutions, types, interactions of light). In this summary, I’ll focus on the latter, mainly defining some of the key characteristics of sensors (Image 1). To have a clearer understanding of these concepts, I will exemplify them with information from two of the main satellites discussed in class: Landsat and Sentinel (Image 2).\nImage 1: Mind map of main characteristics of sensors\nImage 2: Mind map comparing main characteristics of two of the main sensors: Landsat and Sentinel\nFrom this comparison, it’s worth noticing that overall, Sentinel shows better characteristics, like spatial resolutions with more detail, more spectral bands and more frequent temporal resolution than Landsat. It would be interesting to see in which cases, despite having seemingly lower characteristics, Landsat imagery could represent a better tool to assess an spatial problem.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>An Introduction to Remote Sensing</span>"
    ]
  },
  {
    "objectID": "W2_Portfolio.html",
    "href": "W2_Portfolio.html",
    "title": "3  Portfolio tools: Xaringan and Quarto",
    "section": "",
    "text": "This week we explored how tools like Xaringan and Quarto can be used to have reproducible presentations and documents, useful for data science projects. As an exercise, I have developed this book using Quarto and a presentation on the Sentinel -2 satellite using Xaringan, which you can see here:\n\n\n\n\n\n\n\n\nAs a reflection on this exercise, I found that learning how to use these tools, specially Xaringan, was quite challenging, and probably not the best option for a simple presentation, but could be really useful when used to display code or data through tables for example, or when using some of the interactive tools that XaringanExtra has.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Portfolio tools: Xaringan and Quarto</span>"
    ]
  },
  {
    "objectID": "W1_Introduction.html#summary",
    "href": "W1_Introduction.html#summary",
    "title": "2  An Introduction to Remote Sensing",
    "section": "3.1 Summary",
    "text": "3.1 Summary\nThis week’s lecture focused on the foundations of Earth Observation (EO) and Remote Sensing. From all the contents of the class, I will focus, on this summary, on the differences between two main sensors: Landsat and Sentinel. This comparison will draw on some of the contents of the lecture, like types of sensors, resolutions, spectral bands, and applications.\n*Pending to upload mind map",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>An Introduction to Remote Sensing</span>"
    ]
  },
  {
    "objectID": "W1_Introduction.html#applications",
    "href": "W1_Introduction.html#applications",
    "title": "2  An Introduction to Remote Sensing",
    "section": "2.2 Applications",
    "text": "2.2 Applications\nSince this week’s summary included a comparison between Landsat and Sentinel sensors, I considered relevant to check literature that compares both satellites’ imagery to evaluate what their main differences are and when could one of them be a better option than the other.\n\n2.2.1 Burn Severity Mapping in North America: comparing Landsat 8 and Sentinel-2\nThis paper by Howe et al (2022) analysed 26 fires in western North America, and found that “Sentinel generally performed as well or better than Landsat for four spectral indices of burn severity”, also that Sentinel’s finer spatial resolution helps to identify fine-scale fire effects and therefore has a more precise identification of spatial patterns of fires and burned areas. One thing they mentioned is that Sentinel’s finer spatial resolution (10m) led to a 5% reduction in high-severity patch interiors compared to Landsat 8 that has a resolution of 30m, that translated into almost 25 000 less mapped hectares. When I first saw the 5% reduction it seemed to me that it has not really a huge impact and that the differences were marginal but after seeing the real influence in area and also how evident it was visually, I understood that that kind of precision, even if little could make a great difference and that overall using Sentinel represents a huge improvement in the accuracy, at least for this kind of applications.\nImage 3: Maps of high-severity patch interior for the 2017 Meyers fire in southwestern Montana (A), and the 2018 Trail Mountain fire in central Utah (B) derived from Landsat (left column) and Sentinel (right column) imagery.\n\n\n\n2.2.2 Analysis of urban heat islands with Landsat satellite images and GIS in Kuala Lumpur Metropolitan City\nWhile reviewing different papers I found it difficult to find one that when comparing both Landsat and Sentinel, opted to use the former. So instead, I decided to look for a paper that since the beginning decided to use it as part of its methodology(so it’s not a comparison, but explains why they opted for this sensor). That’s when I found this paper by Kasniza et al (2023) which explored the evolution of heat islands and their relationship to land surface temperature in Kuala Lumpur using Landsat 8 imagery. Instead of focusing on the research details, I want to focus on the methodology, where they explained that Landsat 8 was chosen because it provides thermal data through Band 10: Thermal Infrared (which isn’t available on Sentinel), which was later used to calculate the Urban Heat Island index, alongside other bands like NIR.\nImage 4: Flowchart of the process of obtaining the urban heat indexes, showing which bands where used.\n\nAfter checking these two papers and also reviewing some others, I can tell that it’s true that Sentinel 2 can be a great option(and very popular as well, maybe also because its newer?) because of its spatial and temporal resolution, for many types of applications that could be analysed with its available bands. But we can’t generalize it as THE best option as, sometimes, like in the second paper, we might require some kind of information that only Landsat has, like in this case Thermal Infrared.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>An Introduction to Remote Sensing</span>"
    ]
  },
  {
    "objectID": "W1_Introduction.html#reflections",
    "href": "W1_Introduction.html#reflections",
    "title": "2  An Introduction to Remote Sensing",
    "section": "2.3 Reflections",
    "text": "2.3 Reflections\nThis week I learnt a lot of the foundations behind sensors which I actually didn’t expected(I thought we would focus more directly on applications of satellite data) but I found interesting to get a grasp on what’s actually going behind the images we see. Having an architecture background, I have previously used Satellite Images and I have “modified” them using programs like Photoshop and I had also the notion that images are just a combination of colors (like RGB), but I never expected to see that actually images are different layers of numbers that we can filter or modify to change what we see (now in hindsight, feels so obvious haha). Now I think I’ll always see them in a whole new way!\nNow moving on to this weeks’ entry topic I liked seeing the possibilities that each sensor has and also realizing that sometimes being newer or having better resolutions doesn’t really make it the absolut BEST SENSOR. After the lecture I was certain that Sentinel had more advantages(more options for example) in terms of its various resolutions and uses. However, after reviewing literature and considering the pros and cons of each, I think Landsat is still highly relevant, especially to certain topics where they have bands that others don’t, for example when it comes to analyzing heat, as seen on the second paper. Also, I was trying to think in which other cases it could be the best option and I realized that even though people are really into Sentinel also because it’s relatively new, that might actually be a good reason not to use it. For example, if I were looking to analyze long-term trends, Landsat would probably be the best option as it will have a larger range(time wise) of data, which would help me compare changes over time more accurately. Overall, the main takeaway from this week’s learning diary entry would be that there’s not a single better sensor, and that choosing to use one will depend on the kind of research I want to do, so it’s worth assessing first what kind of information we need and then see which sensor can helps us with what we need, and for that, looking for examples in previous research is a great tool.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>An Introduction to Remote Sensing</span>"
    ]
  },
  {
    "objectID": "W3_RemoteSensingData.html",
    "href": "W3_RemoteSensingData.html",
    "title": "4  Remote Sensing Data",
    "section": "",
    "text": "4.1 Summary\nThis week introduced a lot of new concepts related to corrections, data joining, and enhancements in remote sensing. To keep things clear for future reference, I organized them into categories and decided to do kind of a “dictionary” of these concepts, so it will be useful to understand them when reviewing literature and seeing applications of them.\nImage 1. Quick summary of main concepts mentioned in class this week",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Remote Sensing Data</span>"
    ]
  },
  {
    "objectID": "W3_RemoteSensingData.html#applications",
    "href": "W3_RemoteSensingData.html#applications",
    "title": "4  Remote Sensing Data",
    "section": "4.2 Applications",
    "text": "4.2 Applications\n\n4.2.1 Multisensor fusion: pansharpening example\nThis paper by Siok et al.(2020),\nImage 3. On the left samples of the best high-spatial-images. On the right, samples of images in natural color composition after being enhanced with the best high-spatial-resolution image\n\n\n\n\n\nThese corrections and enhancements are essential for making sure remote sensing data is accurate and actually useful. For example, atmospheric corrections are super important for environmental monitoring, where we need consistent multi-temporal comparisons. Geometric corrections are crucial for historical imagery alignment, especially in change detection studies. Radiometric calibration ensures that sensor data is comparable across time and space.\nEnhancements also have a ton of applications in urban studies, agriculture, and disaster response. Band ratios (like NDVI) help in assessing vegetation health, while texture analysis is great for land-use classification. PCA is often used for detecting land-cover changes, highlighting major differences in multi-temporal images. Image fusion techniques improve spatial resolution, making high-precision mapping and infrastructure monitoring more effective.\nOne interesting example is multi-sensor fusion, where high-resolution panchromatic images enhance low-resolution multispectral data to improve feature detection. A study titled Multi-Sensor Fusion: A Simulation Approach to Pansharpening Aerial and Satellite Images (MDPI, 2020) discusses different ways to sharpen images and extract better features using fusion techniques.\n\n\n4.2.2 Textural images for improving land-cover classification in the Brazilian Amazon\nThis paper by Lu et al. (2014)\nImage 4. Illustration done to summarise the main terms used in Remote Sensing",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Remote Sensing Data</span>"
    ]
  },
  {
    "objectID": "W3_RemoteSensingData.html#reflections",
    "href": "W3_RemoteSensingData.html#reflections",
    "title": "4  Remote Sensing Data",
    "section": "4.3 Reflections",
    "text": "4.3 Reflections\nAt first, learning about corrections and enhancements felt unnecessary as most modern datasets come preprocessed anyway. But after seeing the applications I think its worth knowing the possibilities we have and what we could do if we ever encounter raw data(maybe not even satellite data). Even if we end up using only Analysis-ready data, I now think it’s important to know what kind of process it has been through so we can assess it better and know what we are working with. Even though I now realise its importance, I found this week to be a bit overwhelming because of all the concepts, but after organizing everything, it started making sense. Some papers are still hard to follow with all their complex formulas, but at least now I have a better foundation to understand what’s going on, so I guess we are having progress!\nA couple things caught my attention this week. One thing that surprised me was how much regression is used in remote sensing. I always thought of regression as something for statistical modeling (like in CASA007), but it’s actually everywhere here, aligning images, calibrating radiance, enhancing quality, etc. The other one was seeing Andy’s fieldwork for atmospheric correction which seems pretty cool. It made me think that sometimes we might need higher levels of precision for our images and its interesting to see all the options we have available to handle that.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Remote Sensing Data</span>"
    ]
  },
  {
    "objectID": "W4_Policy.html",
    "href": "W4_Policy.html",
    "title": "5  An Introduction to Remote Sensing",
    "section": "",
    "text": "5.1 Summary\nThe Plan Integral de Reconstrucción con Cambios (PIRCC) is a strategy developed by the Peruvian government to rebuild and strengthen areas affected by El Niño Costero. Its goal is not just to repair damaged infrastructure but also to make communities more resilient against future disasters. The plan promotes sustainable urban development and better risk management, recognizing that prevention is more effective than post-disaster reconstruction. Also, given the increasing impact of climate change, the PIRCC highlights the need to prepare for extreme weather events rather than just reacting to them.\nSource\nSource\nThat said, while the plan includes prevention measures, these have mostly focused on building flood defenses, dikes, and urban development plans for affected cities, but is missing a set of tools to better inform the location of these projects from a data-driven approach, and that overall, allow long-term risk monitoring, including tools that could provide real-time data and analysis to guide urban planning and infrastructure decisions. Without that, rebuilding efforts might not be too strategic, and could have less accuracy, reducing that way its effectiveness to prevent future natural phenomena.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>An Introduction to Remote Sensing</span>"
    ]
  },
  {
    "objectID": "W4_Policy.html#applications",
    "href": "W4_Policy.html#applications",
    "title": "5  An Introduction to Remote Sensing",
    "section": "5.2 Applications",
    "text": "5.2 Applications\nConsidering this, and in order to assist with contributing to PIRCC’s policies goals, I consider it would be relevant to create a monitoring tool using Landsat imagery(because of its long-term data) that can analyse previous years’ affected areas by El Niño and with that information classify land risk. These data can be used to identify regions with repeated flooding, land degradation, and other vulnerability factors that are indicative of high-risk areas.\nFor that, spectral indices such as NDVI (Normalized Difference Vegetation Index) and NDBI (Normalized Difference Built-up Index) could be used to assess land cover changes and urban expansion, helping to identify areas where improvements are most needed.\nWith this application, we could shift towards a more data-driven approach to disaster prevention that could be replicated in different areas affected by this phenomenon, and ultimately, helps better inform policies to maximize its effects.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>An Introduction to Remote Sensing</span>"
    ]
  },
  {
    "objectID": "W4_Policy.html#reflections",
    "href": "W4_Policy.html#reflections",
    "title": "5  An Introduction to Remote Sensing",
    "section": "5.3 Reflections",
    "text": "5.3 Reflections\nThrough this brief, I could understand how to link what we are learning on Earth Observation, to planning urban development, which is related to my background as an urban planner. Also, having a critical analysis of the policy, it made me realize that they are lacking some important data that could improve its effectiveness.\nThrough the application, I consider satellite imagery can be leveraged to address specific challenges posed by El Niño, and can better anticipate future risks, making the PIRCC more effective and sustainable in the long term.\nAdditionally, through this exercise, I’ve recognized the importance of collaboration between different data sources, such as satellite imagery and ground-level assessments, to create a comprehensive view of the situation. Also, it’s clear that while remote sensing can provide invaluable insights, it must be combined with local knowledge and community engagement to ensure that the data is used effectively in shaping policies and practices on the ground.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>An Introduction to Remote Sensing</span>"
    ]
  },
  {
    "objectID": "W6_GEE.html",
    "href": "W6_GEE.html",
    "title": "6  Google Earth Engine",
    "section": "",
    "text": "6.1 Summary\nThis week, we finally started using Google Earth Engine (GEE), and honestly, it felt like a game changer compared to earlier tools like SNAP. GEE makes satellite image analysis way more practical and less time-consuming. We went through the basics: how to access and manipulate satellite imagery, filter and scale values, join images, clip them, and even perform Principal Component Analysis (PCA).\nOne thing to mention is that GEE runs on JavaScript, which sounds intimidating at first, but so far, we’ve only used relatively simple commands, so it’s not that bad. I have summarized some of the main things to remember when using Javascript in GEE, in the following image:\nAlso, some words to keep in mind when working in GEE have been summarized here:\nA key takeaway is that using GEE’s built-in server side functions is way better performance wise. So, for example, instead of writing traditional loops, we should use functions like .map() to speed things up as they have been optimized for GEE.\nOther important concepts:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Google Earth Engine</span>"
    ]
  },
  {
    "objectID": "W6_GEE.html#applications",
    "href": "W6_GEE.html#applications",
    "title": "6  Google Earth Engine",
    "section": "6.2 Applications",
    "text": "6.2 Applications\nGEE’s massive satellite data catalog opens the door for a huge range of applications. We can do everything from climate studies to nighttime light analysis, with global coverage for most datasets. This is especially valuable for regions in the Global South, where detailed geospatial datasets are often lacking. With GEE, we can bypass that limitation and still conduct meaningful research and analysis.\n\n\n\n\n\nSource\nSome key application areas:\n\nEnvironmental Monitoring: Analyzing land cover changes, deforestation, and water body dynamics.\nUrban Studies: Monitoring urban expansion, heat islands, and transportation networks.\nAgriculture & Vegetation: Using NDVI and other indices to assess crop health and vegetation patterns.\nDisaster Response: Mapping flood-prone areas, tracking wildfires, and assessing damage after natural disasters.\nAir Quality & Climate: Analyzing pollution patterns and long-term climate trends.\n\nThere’s also the possibility of integrating GEE with machine learning models for tasks like land classification and object detection. And on top of all that, GEE even allows us to build interactive web applications, definitely something I am looking to try as I am also taking the CASA25 module.\n\n6.2.1 Analysis of changes in rivers planforms using GEE\nThis paper by Tobón and Cañón (2019)\n\n\n\n\n\nFigure 6. Example of methods for surface water delimitation in two river reaches: AWEI in (a) Landsat and (b) Sentinel; NDWI in (c) Landsat and (d) Sentinel; MNDWI in (e) Landsat and (f) Sentinel. The same quadrants used for Landsat and Sentinel",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Google Earth Engine</span>"
    ]
  },
  {
    "objectID": "W6_GEE.html#reflections",
    "href": "W6_GEE.html#reflections",
    "title": "6  Google Earth Engine",
    "section": "6.3 Reflections",
    "text": "6.3 Reflections\nSwitching to GEE made everything feel much more efficient. Compared to previous weeks, loading and manipulating large datasets (even for the whole world) was a breeze. The user interface of GEE was a bit overwhelming at first, but that’s probably just because it was my first time using it, but overall it seems that it offers many functionalities that I am looking forward to try in the next weeks. On that line, the GEE catalog made me realize how much data we have available and how using Remote Sensing is democratizing the access to many datasets that will definitely benefit research and data-informed policy making especially in countries where data availability is usually an issue, like my country. I think the possibilities are endless now!\nOn another topic, when Ollie mentioned that even though GEE has been around for over a decade, Google could decide to shut it down at any time, it made me think that it would be such a huge loss especially after seeing how much research has been done since it was launched, but it also made me think that then it makes sense that we didn’t go straight to it(even though that’s what I was initially expecting) but instead we first focused on learning the theoretical concepts and understanding the logic behind what’s happening in the background. I think this way, even if in the future we have to change to another program, we will not be starting from scratch as we now know the foundations of it.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Google Earth Engine</span>"
    ]
  },
  {
    "objectID": "W8_Classification2.html",
    "href": "W8_Classification2.html",
    "title": "8  Classification 2",
    "section": "",
    "text": "8.1 Summary\nThis session could be divided in two main topics. On one side (and also the one i felt was easier to understand) was continuing seeing alternatives to image classification, including some preclassified datasets like MODIS or Dynamic World. The other part (which for me was a bit difficult to understand) covered key concepts in classification accuracy assessment, cross-validation techniques, the impact of spatial autocorrelation, and spatial cross validation.\nFor the first part, we got deeper on Dynamic World which is a open-access semisupervised model with already preprocessed data to obtain info on tree cover, built index, etc, that is continously being updated. Something interesting it that it uses Convolutional Neural Networks, which is a form of deep learning that uses a moving window for classification. Some cons would be that its not too detailed, has some inaccuracies because of using TOA reflectance and lacks interpretability.\nWe also saw alternatives to classification techniques like:\nOBIA: Object based image analysis : instead of classifying cells, segments images in meaningul shape based on similarity, called superpixels and then classifies. It uses SLIC which is a method to refine classification that checks for homogenity of colors and closeness to center\nSubpixel analysis: recognizes that one pixel might include multiple land cover types so we could calculate the proportion of each pixel corresponding to each class. it is computed usingmatrix inversion techniques, with each fraction summing to one.\nThen moving to accuracy assessment. First of all I found this table to be the basis of all next concepts\n*Graph on true positive, true negative, false positive, false negative\nSo, there are many indexes to measure accuracy\nPA: Producer accuracy : true positive and false negative rate\nUA: User accuracy: true positive and false positive\nOA: Overall accuracy: true positive and true negative compared to all classifications\nErrors of omission: 100 - PA\nErrors of Comission: 100 - UA\nKappa coefficient: accuracy of an image compared to the results by chance. Its use is controversial\nF1 measure: combines PA and UA but doesn’t take in account true negatives\nConsiderations: its not possible to maximize both producer and user accuracy at the same time.\nHow do we get test data for accuracy assessment?\nModels need to be tested on different datasets to ensure they generalize well\nBasic validation splits the data into training and testing subsets\nCross-validation improves reliability by repeating the process multiple times, commonly using k-fold (e.g., 10-fold) validation, where data is split into k parts and rotated between training and testing.\nA more extreme form, leave-one-out cross-validation, tests each data point separately, though it’s rarely used in remote sensing due to high computational costs\nWe also saw Spatial Autocorrelation and things to consider about it:\nSpatial autocorrelation means that data points close to each other are more similar than those farther apart (Tobler’s Law: “everything is related, but near things are more related”).\nIf training and test data are too close, the model sees a “sneak preview” of test data, leading to an overestimated accuracy.\nTraditional pixel-based accuracy assessments often suffer from this issue.\nSpatial cross validation\nUnlike standard cross-validation, spatial cross-validation ensures that training and test data are spatially separated.\nThis reduces the effect of spatial autocorrelation and provides a more realistic accuracy estimate.\nMethods include:\nObject-based classification: Instead of individual pixels, entire objects (e.g., buildings or vegetation patches) are classified.\nSpatial partitioning: Training and test data are assigned to separate geographic areas to avoid overlap.\nSome main takeaways:\nAlways check if spatial autocorrelation is considered in model validation; ignoring it inflates accuracy.\nCross-validation improves reliability but needs spatial modifications to work well in remote sensing.\nHyperparameters should be tuned using spatial cross-validation to ensure robustness.\nIf someone presents a machine learning model using spatial data, ask if they accounted for spatial dependencies—otherwise, their accuracy might be overestimated.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Classification 2</span>"
    ]
  },
  {
    "objectID": "W8_Classification2.html#applications",
    "href": "W8_Classification2.html#applications",
    "title": "8  Classification 2",
    "section": "8.2 Applications",
    "text": "8.2 Applications\nUnderstanding spatial cross-validation is crucial for improving model reliability in Earth observation, remote sensing, and spatial machine learning. In practical applications:\nEarth Observation & Remote Sensing: Ensuring classification models for land cover or environmental monitoring are robust and not overly optimistic.\nUrban Studies & Mobility Analysis: Avoiding biased results in models predicting foot traffic, land use changes, or transport patterns.\nHyperparameter Tuning: Optimizing parameters in machine learning models (e.g., SVMs) while maintaining spatial integrity in training and validation.\nBig Data Applications: When dealing with large geospatial datasets, proper validation methods help in making reliable predictions applicable to policy and planning.\n\n8.2.1 Mapping informal settlement indicators using object-oriented analysis in the Middle East\nThis paper by Fallatah et al.(2017)\n\n\n\n\n\n\n\n8.2.2 Mapping vegetation in the Amazonian wetlands using object-based image analysis and decision tree classification\nThis paper by De Oliveira and Rossetti (2014)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Classification 2</span>"
    ]
  },
  {
    "objectID": "W8_Classification2.html#reflections",
    "href": "W8_Classification2.html#reflections",
    "title": "8  Classification 2",
    "section": "8.3 Reflections",
    "text": "8.3 Reflections\nThis week was dense. The topics where kind of abstract and sometimes I couldn’t fully understand what was going on, but i got to understand the main ideas behind it. Doing the summary and reviewing the tables helped me to understand more but in general I think I’ll understand it better once I start calculating my own accuracy in exercises. Still I am not sure if I’ll get to need to do this kind of accuracy assessments at the most in-depth level once I graduate, but I think its a good idea to understand the basics so in case I encounter them in any paper or project I can understand what they mean and also not be so naive to believe in super high accuracies and always question what they really are assessing.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Classification 2</span>"
    ]
  },
  {
    "objectID": "W7_Classification.html",
    "href": "W7_Classification.html",
    "title": "7  Classification",
    "section": "",
    "text": "7.1 Summary\nThis week we explored machine learning techniques and reviewed some remote sensing applications for them(like mapping urban sprawl, illegal logging, and land cover classification). Most of these concepts were completely new to me, so it was a great introduction to them and all the possibilities we have. Because it was my first time hearing of some of these methods, for this week’s entry I decided to structure/categorise all the methods mentioned and include what they are and how they can be useful. Also, I organized kind of a dictionary to explain some of the main concepts that were used to talk about machine learning so I can have them for future reference.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "W7_Classification.html#applications",
    "href": "W7_Classification.html#applications",
    "title": "7  Classification",
    "section": "7.2 Applications",
    "text": "7.2 Applications\nMachine learning is a game-changer for remote sensing, particularly in regions where traditional data collection is difficult or expensive. Some key applications include\nUrban Growth & Land Cover Classification: Identifying informal settlements, urban expansion, or deforestation using satellite imagery. Supervised classification methods like SVM or Random Forests are commonly used for this.\nDisaster Response & Environmental Monitoring: Mapping flooded areas or monitoring forest loss. Unsupervised methods like K-means can be used to detect changes over time.\nAgriculture & Food Security: Classifying different crop types, predicting yields, or assessing drought impacts. Machine learning models can process multispectral images to detect vegetation health.\nClimate Change Studies: Analyzing glaciers, urban heat islands, or desertification trends using historical and real-time satellite data.\nData-Sparse Regions: In Global South countries, official land-use maps may be outdated or nonexistent. Machine learning allows researchers to generate high-quality maps from remote sensing data, filling gaps in public datasets.\nWhat stood out to me is how machine learning makes it possible to analyze and classify areas that might otherwise be impossible to map manually. In cities with rapid urbanization but little official data, these techniques can provide real-time insights that governments and researchers can act upon.\n\n7.2.1 Detecting industrial oil palm plantations on Landsat images with Google Earth Engine\nThis paper by Huay et al.(2016)\n\n\n\n\n\n\n\n7.2.2 Mapping Informal Settlements in Developing Countries using Machine Learning and Low Resolution Multi-spectral Data\nThis paper by Gram-Hansen et al.(2019)\n\n\n\n\n\nPredictions of informal settlements (white pixels) in Kibera, Nairobi. Left: The CCF prediction of informal settlements in Kibera on low-resolution Sentinel-2 spectral imagery. Middle: Deep learning based prediction of informal settlements in Kibera, trained on VHR imagery. Right: The ground truth informal settlement mask for Kibera.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "W7_Classification.html#reflections",
    "href": "W7_Classification.html#reflections",
    "title": "7  Classification",
    "section": "7.3 Reflections",
    "text": "7.3 Reflections\nThis week was honestly mind blowing. Since I’m not taking the CASA06 module (which goes deeper into ML), I hadn’t really explored all the different Machine Learning and classification techniques before. This felt like a great introduction to not just to the methods themselves, but also to how they can be applied to remote sensing.\nBefore this, I saw Machine Learning as something super advanced and kind of out of my league. But after this class, I realized I’ve actually been using it all along (like with something as simple as Linear Regression). It’s also funny to see all the current hype around AI and ML when in reality, a lot of these models are just statistical methods that have been around for decades. That said, I also got a sense of how many more complex approaches exist, and I hope I can explore them later in more advanced applications!\nOne key takeaway (which also ties back to something Jon mentioned in one of the first FSDS lectures) is that just because we have all these sophisticated models available doesn’t mean we should always go for the most complex one. Sometimes, a simpler model is the better choice, especially since accuracy and complexity often come at the cost of interpretability. This is particularly important in areas like policy and urban planning (which is what I’m aiming to work in), where having a clear, explainable model can be more valuable than one that’s just technically precise.\nOverall, this week’s content was super useful, and I’m really excited to apply it in GEE, my other modules, and even my dissertation!",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "W9_SAR.html",
    "href": "W9_SAR.html",
    "title": "9  Synthetic Aperture Radar",
    "section": "",
    "text": "9.1 Summary\nThis week, we explored SAR data which I found to be quite different from optical sensors. First of all, SAR is an active sensor (so far all of them have been passive), which means that it emits a signal (like a bat) and measures the reflected signal. Also, unlike optical imagery, this one produces black and white images, ranging only from low to high values of the amplitude of the signal reached by the sensor, making it more about texture than color.\nSimilarly to optimal imagery, it has multiple bands, each one being the result of different frequencies of microwaves. Some of these frequencies could even penetrate through thin canopies and atmospheric occlusions like clouds, but offer lower resolution. These are some of the bands:\nC band: The most useful and used SAR band (e.g. Sentinel 1). Has a good balance of resolution and penetration and generates volume scattering\nX band: Has a shorter wavelength so it’s very easy to scatter meanign it just bounces and generates a rough surface\nL band and P band: Has a longer wavelength. It can penetrate through some objects and generate double bounce scattering\nSAR also has polarizations, which is the orientation of the plane in which the waves move. Meanwhile, objects have a scattering mechanism, which is the way that objects reflects the radar signal. Scattering mechanisms are a really important dimension in SAR, analogous to the color of images in optical imagery. Their types can be:\nPolarization:\nScattering mechanisms:\n*Currently we only have access to VH and VV in Sentinel 1\nThe types of information we get from SAR sensor are amplitude (backscatter) and phase\nAlthought SAR is a bit less useful for classification, one of its biggest strengths is change detection. Since SAR isn’t affected by cloud cover or lighting conditions, it’s more consistent and provides a reliable way to track changes in landscapes and infrastructure. A basic approach is subtracting two SAR images to highlight differences, though this doesn’t work well in areas with constant change (e.g. construction sites). Another way of approaching it was Ollie’s research on building damage detection using SAR. His method analyzed the variance in backscatter before and after a war, identifying changes outside the normal range of variation by evaluating the standard deviation of each pixel before and after war to detect changes outside of the normal noisiness/change. It was interesting to see that his statistical change detection, even though is simpler than using other Machine Learning methods, had a better performance, but makes sense as in this case of crisis scenarios you usually don’t have pretrained data that can help calibrate ML models.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Synthetic Aperture Radar</span>"
    ]
  },
  {
    "objectID": "W9_SAR.html#applications",
    "href": "W9_SAR.html#applications",
    "title": "9  Synthetic Aperture Radar",
    "section": "9.2 Applications",
    "text": "9.2 Applications\nSAR has a broad range of applications, especially for monitoring environmental changes, infrastructure, and disasters. Some key uses include:\nDeforestation tracking – SAR can detect forest loss even under cloud cover.\nFlood monitoring – Water surfaces have a distinct SAR signature, making it useful for disaster response.\nUrban development mapping – Double-bounce scattering helps identify buildings and structural changes.\nGlacier and ice sheet analysis – SAR can penetrate snow and ice to monitor thickness changes.\nConflict and disaster damage assessment – As seen in Ollie’s work, SAR can detect structural damage in war zones or after natural disasters.\nOne area that really caught my interest is hydrology. I’ve previously analyzed river changes over time and noticed seasonal variations. SAR could provide a new way to study this, potentially detecting flood patterns and long-term riverbed shifts. However, this would likely require phase data, which isn’t available in GEE, so I’d have to explore other tools..\n\n9.2.1 Detection of flooded urban areas in high resolution Synthetic Aperture Radar images using double scattering\nThis paper by Mason et al.(2014)\n\n\n\n9.2.2 Mapping and Monitoring Surface Water and Wetlands with Synthetic Aperture Radar\nThis paper by Brisco (2015)\n\nChange in double-bounce scattering between June 6 and August 17, 2008, RADARSAT-2 polarimetric data for Dongting Lake, China and the surrounding area. This is due to the change in SAR scattering mechanisms as the water level drops after snowmelt runoff ceases and the vegetation is no longer flooded.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Synthetic Aperture Radar</span>"
    ]
  },
  {
    "objectID": "W9_SAR.html#reflections",
    "href": "W9_SAR.html#reflections",
    "title": "9  Synthetic Aperture Radar",
    "section": "9.3 Reflections",
    "text": "9.3 Reflections\nThis week was super interesting because SAR introduces a completely different way of analyzing Earth’s surface compared to optical imagery. Before coming to UCL, I hadn’t even heard of SAR, so it’s exciting to dive into something new and realize how useful it is.\nOne thing that stood out(also aligned with one of my reflections weeks ago) was how more complex models aren’t always better. Ollie’s study showed that deep learning, despite all the AI hype, wasn’t the best option for detecting building damage and that his statistical approach was actually more reliable. This made me think that the best option its actually to really understand what are all the possibilities each option bring us and evaluate which makes more sense for each case depending on the available data and expected outputs, and that at the end, choosing the right tool is more important than just picking the most advanced one.\nIt was also great to try SAR in GEE and see how all these concepts translate into real world analysis. I quite liked the focus Ollie has on his practicals, using it to assess serious urban and social issues which makes me think that what we are learning can have a greater impact on society(something I have been trying to incorporate when deciding what to do for my dissertation as I am currently also about to present my dissertation proposal haha).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Synthetic Aperture Radar</span>"
    ]
  },
  {
    "objectID": "W6_GEE.html#summary",
    "href": "W6_GEE.html#summary",
    "title": "6  Google Earth Engine",
    "section": "",
    "text": "GEE Elements\nMeaning\n\n\n\n\nImage\nRaster\n\n\nFeature\nVector\n\n\nImageCollection\nA stack of images\n\n\nFeatureCollection\nA stack of features\n\n\nClient side\nFront end: The part that users interact with\n\n\nServer side\nBack end: Where the data is retrieved and processed\n\n\n\n\n\n\nScale & Resolution: GEE aggregates data based on zoom level—so the further you zoom out, the bigger each pixel represents. Scale and resolution are always linked!\nProjection: Everything is converted to the Mercator projection by default.\nFiltering: To avoid loading excessive data, we always filter by time and spatial bounds.\nOperations & Applications: GEE lets us perform geometric operations, corrections, enhancements, and even advanced tasks like machine learning, classification, and deep learning.\n\n\n6.1.1 Practical\nIn the practical, we applied these concepts to the city of Delhi, but I also decided to analyze a Peruvian city following the same steps. It was interesting to apply the techniques in a familiar context, though I encountered limitations, such as needing to increase the cloud presence threshold to obtain enough images. But overall it was amazing to finally use GEE and see how fast and practical it was to load the data for anywhere in the world and run the analysis.\nImage 1: PCA analysis of Iquitos, Peru",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Google Earth Engine</span>"
    ]
  },
  {
    "objectID": "W3_RemoteSensingData.html#summary",
    "href": "W3_RemoteSensingData.html#summary",
    "title": "4  Remote Sensing Data",
    "section": "",
    "text": "4.1.1 Key terms\nSo far many words have been repeated multiple times during lectures and papers I have reviewed and sometimes I still mix them up, so here they are summarised:\n\n\n\n\n\n\n\nTerm\nMeaning\n\n\n\n\nIrradiance\nRadiation reaching Earth from the Sun\n\n\nRadiance\nRadiation leaving Earth toward the satellite\n\n\nReflectance\nThe proportion of incoming radiation reflected by a surface\n\n\nSurface Reflectance\nReflectance at the bottom of the atmosphere (after atmospheric correction)\n\n\nAnalysis-ready data\nPreprocessed images, usually in Surface Reflectance format (so we don’t have to do all the corrections ourselves!)\n\n\n\nImage 2. Illustration done to summarise the main terms used in Remote Sensing\n\n\n\n4.1.2 Corrections:\nThis week we saw some of the corrections that raw sensor data could use before we start manipulating it. Most of them are already applied to images we use, as we usually use Analysis-ready data, but I think knowing what kind of preprocessing images have is useful at least as a general knowledge or if we were to work on raw data(maybe needed if I were to work with drone imagery?).\n\n\n\n\n\n\n\nTerm\nMeaning\n\n\n\n\nGeometric correction\nAligns misaligned images using control points and regression (basically shifting everything so it matches properly). Useful for correcting historical maps to match modern spatial data.\n\n\nAtmospheric correction\nRemoves distortions caused by atmospheric scattering (because the atmosphere its always there!)\n\n\nDark Object Subtraction (DOS)\nMethod for atmospheric correction that finds something that should have zero reflectance and subtracts its measured value from all pixels\n\n\nPseudo-Invariant Features (PIFs)\nMethod for atmospheric correction that uses objects with stable reflectance over time as references for correction (great for long-term studies)\n\n\nFlat correction\nMethod for atmospheric correction that uses field-measured values (without atmospheric interference) as a baseline for regression\n\n\nOrthorectification\nCorrection that fixes distortions from sensor tilt\n\n\nRadiometric calibration\nConverts Digital Number (DN) to spectral radiance to standardize measurements\n\n\n\n\n\n4.1.3 Enhancements:\nI would say enhancements are a way of transforming images using data that is already available in the image itself, to better show some features. Some of the methods are:\n\n\n\n\n\n\n\nTerm\nMeaning\n\n\n\n\nContrast adjustments\nModifies image appearance without changing data values (basically, just making it look better)\n\n\nBand ratios\nDividing one band by another (e.g., NDVI for vegetation analysis)\n\n\nFiltering\nCan be low-pass (smoothing) or high-pass (highlighting edges, like for detecting buildings)\n\n\nPrincipal Component Analysis (PCA)\nReduces dimensionality to capture the most variance (helps focus on key changes in images, but could loose some interpretability)\n\n\nTexture analysis\nMeasures similarity between a pixel and its neighbors (useful for spotting urban areas or specific land features)\n\n\nImage fusion & pansharpening\nMerges data from different sensors, often using high-resolution bands to sharpen lower-resolution images",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Remote Sensing Data</span>"
    ]
  },
  {
    "objectID": "W7_Classification.html#summary",
    "href": "W7_Classification.html#summary",
    "title": "7  Classification",
    "section": "",
    "text": "7.1.1 Some foundational concepts:\nHuman learning: uses inductive learning, so given our experience of the world, we make inferences on the images or data we see\nExpert system: system that uses human knowledge as a base to solve problems. Tries to show a computer how humans reach decisions\nMachine learning: science of computer modeling of learning process trying to replicate inductive learning\nTwo schools: On one side, Traditional classifiers, they don’t apply a model, they just divide the data based on a feature space. On the other side, the newer methods use Machine Learning.\n\n\n7.1.2 Classification methods:\nLinear Regression: Predicts continuous values by finding the best fit between independent and dependent variables.\nDecision Trees (CART): Useful when linear regression assumptions don’t hold. Classification trees assign discrete categories, while regression trees predict continuous values by recursively splitting the data.\nRandom Forests: Multiple decision trees working together, increasing accuracy but reducing interpretability. The model votes on the most likely classification, which makes it more reliable but harder to understand.\nClustering/K-means: Unsupervised method, similar to DBScan, it makes clusters depending on similarities with other pixels. Useful when categories are unknown\nISODATA: Extension of K-means. Similar but has some inputs called hyperparameters\nMaximum likelihood, density slicing, parallelpiped : They are more classical methods not used that much nowadays. They are supervised methods that start with a class definition, selects the training data, and applies the model to the rest of the data.\nSupport Vector Machine SVM: Comparable to linear regression. Instead of a line there’s a hyperplane and many support vectors . This hyperplane separated two classes but allows some misclassificationndefined(soft margin). It’s a two class comparison but could be replicated for multiple classes.\nNeural Networks & Deep Learning: We didn’t cover these in detail, but they represent the next level, more powerful but often a “black box,” meaning high accuracy at the cost of interpretability.\n\n\n7.1.3 Machine learning concepts:\nOverfitting: When the model fits the training data too well that makes it useless for new data. In order to avoid it we have to check for low bias (difference between predicted value and true value) and low variance (difference in accuracy between train fit and test fit)\n\n\n\n\n\nTrain and test data: we develop the model with train data and it doesnt see the test data. Then I use that model to predict other pixels including validating(test) pixels. With this comparison i can assess the accuracy(which we will learn more next week)\nCross-validation: same process but changing what is your train and test data each iteration. It ensures a more robust model.\nBootstrap sampling: sampling by replacement. it means some rows of data can be duplicated\nOut of bag sample: rows of data not used for the random forest. It is used like a validation dataset and is useful to get the out of bag error\nSupervised classification: The model learns from labeled training data to classify new data.\nUnsupervised classification: The model classifies data given a method or algorithm. It has no human input and categories are not known a priori\nHyperparameters: control variables\nBlack box: you just know if the model is good but loose interpretability. The most accurate models usually have less interpretability.\nSpatial autocorrelation: if the train and test data are too close spatially, they are probably very similar and the accuracy could be too high but the model not robust enough (overfitting)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "W9_SAR.html#summary",
    "href": "W9_SAR.html#summary",
    "title": "9  Synthetic Aperture Radar",
    "section": "",
    "text": "VV (Vertical-Vertical): Sensitive to rough surfaces and water bodies. *\nVH (Vertical-Horizontal): Detects volume scattering from vegetation. *\nHH (Horizontal-Horizontal): Picks up double-bounce reflections, often seen in urban areas.\n\n\n\nRough surface scattering – Common in open terrain, chaotic reflections. Very visible in VV polarization\nVolume scattering – Occurs in vegetation or complex surfaces. Mostly sensitive in VH polarization\nDouble bounce scattering – Happens with vertical structures like buildings. Mostly sensitive in HH polarization\n\n\n\n\nAmplitude (Backscatter): Measures the “loudness” of the returned signal, useful for texture analysis.\n\n\n\nPhase: Measures when is the reflected signal received. It allows us to measure distance to the ground and change detection over time, but it’s not available in Google Earth Engine (GEE).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Synthetic Aperture Radar</span>"
    ]
  }
]