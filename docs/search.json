[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CASA0023 Learning Diary",
    "section": "",
    "text": "Introduction\nHello world!\nThis Learning Diary is part of the CASA0023 module, Remotely Sensing Cities and Environments, which I’m taking as part of the MSc Urban Spatial Science at UCL. Here, you’ll find weekly entries summarizing key takeaways from each lecture, examples of practical applications, and my personal reflections.\nBut first, let me introduce myself!\n\n\nMy name is Fiorella, and I’m currently pursuing an MSc in Urban Spatial Science. Back in 2021, I graduated as an architect and urbanist in Peru. Since then, I have developed a strong interest in sustainability, climate change, water resources, and urban settlements, particularly how these factors affect urban and natural environments. As an example, during my undergraduate thesis, I focused on assessing changes in a river in the Peruvian Amazon, accelerated by climate change, and proposed solutions to the urban challenges that emerged from these changes.\n\n\n\n\n\nSince graduating, I have worked in the public sector, contributing to Lima’s Rimac River renaturalization project. This role involved urban analysis, master planning, urban design, and policy implementation, with a strong emphasis on risk prevention, and nature-based solutions.\nWhile I have some basic experience using Earth Observation (EO) and remote sensing to address topics like those mentioned above, my approach has been largely empirical. That’s why I find this module to be an exciting opportunity to deepen my knowledge of these tools and processes, enabling me to better inform policies and urban solutions in future projects or research.\nI am particularly interested in how these technologies can be applied to urban challenges in the Global South, where they are often underutilized due to knowledge gaps. That’s why, throughout this diary, my reflections will aim to connect these tools to my background and interests, as well as explore their real-world applications in contexts similar to my country’s.\nAnd if you are curious about my previous projects, here you can have a glimpse at them!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Welcome</span>"
    ]
  },
  {
    "objectID": "W1_Introduction.html",
    "href": "W1_Introduction.html",
    "title": "2  An Introduction to Remote Sensing",
    "section": "",
    "text": "2.1 Summary\nThis week’s lecture covered the basics of Earth Observation (EO) and Remote Sensing including topics such as definitions, relevance and terminology of Earth Observation, as well as more technical concepts to describe sensors (resolutions, types, interactions of light). In this summary, I’ll focus on the latter, mainly defining some of the key characteristics of sensors (Image 1). To have a clearer understanding of these concepts, I will exemplify them with information from two of the main satellites discussed in class: Landsat and Sentinel (Image 2).\nImage 1: Mind map of main characteristics of sensors\nImage 2: Mind map comparing main characteristics of two of the main sensors: Landsat and Sentinel\nFrom this comparison, it’s worth noticing that overall, Sentinel shows better characteristics, like spatial resolutions with more detail, more spectral bands and more frequent temporal resolution than Landsat. It would be interesting to see in which cases, despite having seemingly lower characteristics, Landsat imagery could represent a better tool to assess an spatial problem.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>An Introduction to Remote Sensing</span>"
    ]
  },
  {
    "objectID": "W2_Portfolio.html",
    "href": "W2_Portfolio.html",
    "title": "3  Portfolio tools: Xaringan and Quarto",
    "section": "",
    "text": "This week we explored how tools like Xaringan and Quarto can be used to have reproducible presentations and documents, useful for data science projects. As an exercise, I have developed this book using Quarto and a presentation on the Sentinel -2 satellite using Xaringan, which you can see here:\n\n\n\n\n\n\n\n\nAs a reflection on this exercise, I found that learning how to use these tools, specially Xaringan, was quite challenging, and probably not the best option for a simple presentation, but could be really useful when used to display code or data through tables for example, or when using some of the interactive tools that XaringanExtra has.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Portfolio tools: Xaringan and Quarto</span>"
    ]
  },
  {
    "objectID": "W1_Introduction.html#summary",
    "href": "W1_Introduction.html#summary",
    "title": "2  An Introduction to Remote Sensing",
    "section": "3.1 Summary",
    "text": "3.1 Summary\nThis week’s lecture focused on the foundations of Earth Observation (EO) and Remote Sensing. From all the contents of the class, I will focus, on this summary, on the differences between two main sensors: Landsat and Sentinel. This comparison will draw on some of the contents of the lecture, like types of sensors, resolutions, spectral bands, and applications.\n*Pending to upload mind map",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>An Introduction to Remote Sensing</span>"
    ]
  },
  {
    "objectID": "W1_Introduction.html#applications",
    "href": "W1_Introduction.html#applications",
    "title": "2  An Introduction to Remote Sensing",
    "section": "2.2 Applications",
    "text": "2.2 Applications\nSince this week’s summary included a comparison between Landsat and Sentinel sensors, I considered relevant to check literature that compares both satellites’ imagery to evaluate what their main differences are and when could one of them be a better option than the other.\n\n2.2.1 Burn Severity Mapping in North America: comparing Landsat 8 and Sentinel-2\nThis paper by Howe et al (2022) analysed 26 fires in western North America, and found that “Sentinel generally performed as well or better than Landsat for four spectral indices of burn severity”, also that Sentinel’s finer spatial resolution helps to identify fine-scale fire effects and therefore has a more precise identification of spatial patterns of fires and burned areas. One thing they mentioned is that Sentinel’s finer spatial resolution (10m) led to a 5% reduction in high-severity patch interiors compared to Landsat 8 that has a resolution of 30m, that translated into almost 25 000 less mapped hectares. When I first saw the 5% reduction it seemed to me that it has not really a huge impact and that the differences were marginal but after seeing the real influence in area and also how evident it was visually, I understood that that kind of precision, even if little could make a great difference and that overall using Sentinel represents a huge improvement in the accuracy, at least for this kind of applications.\nImage 3: Maps of high-severity patch interior for the 2017 Meyers fire in southwestern Montana (A), and the 2018 Trail Mountain fire in central Utah (B) derived from Landsat (left column) and Sentinel (right column) imagery.\n\n\n\n\n\n\n\n2.2.2 Analysis of urban heat islands with Landsat satellite images and GIS in Kuala Lumpur Metropolitan City\nWhile reviewing different papers I found it difficult to find one that when comparing both Landsat and Sentinel, opted to use the former. So instead, I decided to look for a paper that since the beginning decided to use it as part of its methodology(so it’s not a comparison, but explains why they opted for this sensor). That’s when I found this paper by Kasniza et al (2023) which explored the evolution of heat islands and their relationship to land surface temperature in Kuala Lumpur using Landsat 8 imagery. Instead of focusing on the research details, I want to focus on the methodology, where they explained that Landsat 8 was chosen because it provides thermal data through Band 10: Thermal Infrared (which isn’t available on Sentinel), which was later used to calculate the Urban Heat Island index, alongside other bands like NIR.\nImage 4: Flowchart of the process of obtaining the urban heat indexes, showing which bands where used.\n\n\n\n\n\n\n\n2.2.3 Some thoughts\nAfter checking these two papers and also reviewing some others, I can tell that it’s true that Sentinel 2 can be a great option(and very popular as well, maybe also because its newer?) because of its spatial and temporal resolution, for many types of applications that could be analysed with its available bands. But we can’t generalize it as THE best option as, sometimes, like in the second paper, we might require some kind of information that only Landsat has, like in this case Thermal Infrared.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>An Introduction to Remote Sensing</span>"
    ]
  },
  {
    "objectID": "W1_Introduction.html#reflections",
    "href": "W1_Introduction.html#reflections",
    "title": "2  An Introduction to Remote Sensing",
    "section": "2.3 Reflections",
    "text": "2.3 Reflections\nThis week I learnt a lot of the foundations behind sensors which I actually didn’t expected(I thought we would focus more directly on applications of satellite data) but I found interesting to get a grasp on what’s actually going behind the images we see. Having an architecture background, I have previously used Satellite Images and I have “modified” them using programs like Photoshop and I had also the notion that images are just a combination of colors (like RGB), but I never expected to see that actually images are different layers of numbers that we can filter or modify to change what we see (now in hindsight, feels so obvious haha). Now I think I’ll always see them in a whole new way!\nNow moving on to this weeks’ entry topic, I think the main takeaway this week is that there’s no single “best” sensor and it all depends on what you’re researching. Before this lecture I would think that Sentinel is definitely better than Landsat because of its different resolutions and uses, but actually after checking some applications I realized Landsat is still super relevant and is now making me think for what other applications it may be better too (maybe long term analysis because Landsat has more time in the market?). Overall, now I know that before picking a sensor, it’s worth thinking about what kind of info I actually need and which sensor is more useful for that specific application, and for that, looking examples of previous research is a great tool!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>An Introduction to Remote Sensing</span>"
    ]
  },
  {
    "objectID": "W3_RemoteSensingData.html",
    "href": "W3_RemoteSensingData.html",
    "title": "4  Remote Sensing Data",
    "section": "",
    "text": "4.1 Summary\nThis week introduced a lot of new concepts related to corrections, data joining, and enhancements in remote sensing. To keep things clear for future reference, I organized them into categories and decided to do kind of a “dictionary” of these concepts, so it will be useful to understand them when reviewing literature and seeing applications of them.\nImage 1. Quick summary of main concepts mentioned in class this week",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Remote Sensing Data</span>"
    ]
  },
  {
    "objectID": "W3_RemoteSensingData.html#applications",
    "href": "W3_RemoteSensingData.html#applications",
    "title": "4  Remote Sensing Data",
    "section": "4.2 Applications",
    "text": "4.2 Applications\nFor this applications section, I decided to look into two ways of enhancing remote sensing data to understand better how to leverage them. One using multisensor fusion (pansharpening) and the other using textural images to enhance land-cover classification. Both methods aimed to get more useful information out of satellite images, but they do it in very different ways.\n\n4.2.1 Multisensor fusion: pansharpening example\nIn this research by Siok et al.(2020), pansharpening was used to enhance the spatial resolution of multispectral images by combining them with higher-resolution panchromatic images. The goal was to improve the clarity and detail of the images while preserving their spectral information. The study compared different pansharpening techniques to assess their effectiveness in maintaining color accuracy and spatial sharpness. Their results showed that certain methods performed better than others, depending on the specific application and the type of satellite data used and also that the effectiveness varied by land cover type, where forests had the least improvement.\nImage 3. On the left samples of the best high-spatial-images. On the right, samples of images in natural color composition after being enhanced with the best high-spatial-resolution image\n\n\n\n\n\n\n\n4.2.2 Textural images for improving land-cover classification in the Brazilian Amazon\nThis study by Lu et al. (2014) explores how textural images from different satellite sensors can improve land-cover classification, particularly in the Brazilian Amazon. The researchers analyzed images from different sensor which have different spatial resolutions, to determine how texture affects classification accuracy. They used a GLCM texture model, and found that adding textural images to radiometric data can enhance classification, especially for high-resolution sensors. Some of their key findings were that not all textures are equally useful and that the best results come from combining two textural images rather than using many, and the ideal texture combinations vary by sensor. Overall, they found that while spectral data remains more effective than texture for distinguishing land-cover types, texture significantly improves classification, especially at higher resolutions.\nImage 4. Textural images with different texture measures for Machadinho d’Oeste; (a), (b), (c), (d), (e), (f), (g), and (h) represent the textural images calculated using mean, variance, homogeneity, contrast, dissimilarity, entropy, second moment, and correlation coefficient, respectively.\n\n\n\n\n\n\n\n4.2.3 Some thoughts:\nBoth studies showed that enhancing remote sensing images, whether by improving spatial resolution or adding texture, can really boost their usefulness. But what really stood out to me is that there’s no “one-size-fits-all” method. Some pansharpening techniques work better than others depending on the land cover, and not all textures are useful for classification or depend on what you are trying to classify or what type of remote sensing data you use. Also, it seems like higher resolution always helps, but only up to a point (so when does more detail stop being helpful?). Overall, it’s cool to see how much thought goes into picking the right enhancement techniques as I would have originally thought that it was something more standard.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Remote Sensing Data</span>"
    ]
  },
  {
    "objectID": "W3_RemoteSensingData.html#reflections",
    "href": "W3_RemoteSensingData.html#reflections",
    "title": "4  Remote Sensing Data",
    "section": "4.3 Reflections",
    "text": "4.3 Reflections\nAt first, learning about corrections felt unnecessary as most modern datasets come preprocessed anyway. But after the lecture I think it’s worth knowing the possibilities we have and what we could do if we ever encounter raw data(maybe not even satellite data). Even if we end up using only Analysis-ready data, I now think it’s important to know what kind of process it has been through so we can assess it better and know what we are working with. Enhancements, instead, felt like something I could be using more in my analysis, and I confirmed it specially after seeing the applications. I found them to be a great toolset to get more information from our images, but also something we need to choose case by case. Now, despite how important these concepts are, I still found this week to be a bit overwhelming because of all the concepts, but after organizing everything, it started making more sense. Some papers are still hard to follow with all their complex formulas, but at least now I have a better foundation to understand what’s going on, so I guess we are having progress!\nOn another topics, a couple things caught my attention this week. One thing that surprised me was how much regression is used in remote sensing. I always thought of regression as something for statistical modeling (like in CASA007), but it’s actually everywhere here, aligning images, calibrating radiance, enhancing quality, etc. The other one was seeing Andy’s fieldwork for atmospheric correction which seems pretty cool. It made me think that sometimes we might need higher levels of precision for our images and its interesting to see all the options we have available to handle that.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Remote Sensing Data</span>"
    ]
  },
  {
    "objectID": "W4_Policy.html",
    "href": "W4_Policy.html",
    "title": "5  Policy",
    "section": "",
    "text": "5.1 Summary\nThe Plan Integral de Reconstrucción con Cambios (PIRCC) is a strategy developed by the Peruvian government to rebuild and strengthen areas affected by El Niño Costero, a meteorological phenomenon that recurrently affects Peru due to the change in the sea temperature. Its goal is not just to repair damaged infrastructure but also to make communities more resilient against future disasters. The plan promotes sustainable urban development and better risk management, recognizing that prevention is more effective than post-disaster reconstruction. Also, given the increasing impact of climate change, the PIRCC highlights the need to prepare for extreme weather events rather than just reacting to them.\nImage 1. Sea surface temperatures on April 4, 2023 in the coast of Peru during El Niño phenomenon.\nSource\nImage 2. Damage caused by excessive rainfall due to the El Niño phenomenon in Peru\nSource\nThat said, while the plan includes prevention measures, these have mostly focused on building flood defenses, dikes, and urban development plans for affected cities, but is missing a set of tools to better inform the location of these projects from a data-driven approach, and that overall, allow long-term risk monitoring, including tools that could provide real-time data and analysis to guide urban planning and infrastructure decisions. Without that, rebuilding efforts might not be too strategic, and could have less accuracy, reducing that way its effectiveness to prevent future natural phenomena.\nImage 3. PIRCC’s budget allocation showing that most of it is dedicated to rebuilding and a very small percentage for development of long-term planning strategies",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Policy</span>"
    ]
  },
  {
    "objectID": "W4_Policy.html#applications",
    "href": "W4_Policy.html#applications",
    "title": "5  Policy",
    "section": "5.2 Applications",
    "text": "5.2 Applications\nConsidering this, and in order to assist with contributing to PIRCC’s policies goals, I consider it would be relevant to create a monitoring tool using Landsat imagery(because of its long-term data) that can analyse previous years’ affected areas by El Niño and with that information classify land risk. These data can be used to identify regions with repeated flooding, land degradation, and other vulnerability factors that are indicative of high-risk areas.\nFor that, spectral indices such as NDVI (Normalized Difference Vegetation Index) and NDBI (Normalized Difference Built-up Index) could be used to assess land cover changes and urban expansion, helping to identify areas where improvements are most needed.\nWith this application, we could shift towards a more data-driven approach to disaster prevention that could be replicated in different areas affected by this phenomenon, and ultimately, helps better inform policies to maximize its effects.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Policy</span>"
    ]
  },
  {
    "objectID": "W4_Policy.html#reflections",
    "href": "W4_Policy.html#reflections",
    "title": "5  Policy",
    "section": "5.3 Reflections",
    "text": "5.3 Reflections\nThrough this brief, I could understand how to link what we are learning on Earth Observation, to planning urban development, which is related to my background as an urban planner. Also, having a critical analysis of the policy, it made me realize that they are lacking some important data that could improve its effectiveness. Through the application, I consider satellite imagery can be leveraged to address specific challenges posed by El Niño, and can better anticipate future risks, making the PIRCC more effective and sustainable in the long term. However I found quite worrying that this huge project, that it using a great amount of the government’s money, didn’t include this kind of remote sensing tools already, and I am wondering how much money we could be saving by making more efficient data-informed decisions.\nAdditionally, through this exercise, I’ve recognized the importance of collaboration between different data sources, such as satellite imagery and ground level assessments, to create a comprehensive view of the situation. Also, it’s clear that while remote sensing can provide invaluable insights, it must be combined with local knowledge and community engagement to ensure that the data is used effectively in shaping policies and practices on the real world.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Policy</span>"
    ]
  },
  {
    "objectID": "W6_GEE.html",
    "href": "W6_GEE.html",
    "title": "6  Google Earth Engine",
    "section": "",
    "text": "6.1 Summary\nThis week, we finally started using Google Earth Engine (GEE), and honestly, it felt like a game changer compared to earlier tools like SNAP. GEE makes satellite image analysis way more practical and less time-consuming. We went through the basics: how to access and manipulate satellite imagery, filter and scale values, join images, clip them, and even perform Principal Component Analysis (PCA).\nOne thing to mention is that GEE runs on JavaScript, which sounds intimidating at first, but so far, we’ve only used relatively simple commands, so it’s not that bad. I have summarized some of the main things to remember when using Javascript in GEE, in the following image:\nImage 1: Main JavaScript features to keep in mind when using GEE\nAlso, some words to keep in mind when working in GEE have been summarized here:\nA key takeaway is that using GEE’s built-in server side functions is way better performance wise. So, for example, instead of writing traditional loops, we should use functions like .map() to speed things up as they have been optimized for GEE. (this was mentioned and emphasized A LOT, so good to keep in mind)\nOther important concepts:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Google Earth Engine</span>"
    ]
  },
  {
    "objectID": "W6_GEE.html#applications",
    "href": "W6_GEE.html#applications",
    "title": "6  Google Earth Engine",
    "section": "6.2 Applications",
    "text": "6.2 Applications\nOne of the things that amazed me the most this week was GEE’s massive satellite data catalog, which opens the door for a huge range of applications. So for this week’s entry instead of focusing on two different research, I decided to change it a bit and first give a general overview of some of the datasets in the catalog that I found more interesting so I can have them on hand for future referece.\nImage 3: Five selected datasets available in GEE\n\n\n\n\n\nSource\nI decided to focus on the GEE Catalog’s category of Human Dimensions. There I found very diverse maps about agriculture, infrastructure and population as the ones I displayed over here. It was interesting to see how diverse these datasets are and how most of them have a global coverage and really small cadence(which I think is valuable for regions like the Global South where detailed geospatial datasets are often not available). At least in this category of the catalog, most of the datasets are already preprocessed, so it’s not only the satellite image but has already had a land classification or object detection which aligns with what was mentioned briefly in the lecture about the possibility of integrating GEE with machine learning models for those tasks(that we will cover later on the module).\nSome key application areas I can think of after seeing the options available are:\n\nEnvironmental Monitoring: identify rapid changes in temperature due to global warming\nUrban Studies: monitoring urban expansion or urban density\nAgriculture & Vegetation: monitoring forest cover, forest degradation\nDisaster Response: mapping affected population and damage after natural disasters or wars\nNatural Resources monitoring: identifying illegal logging, or illegal fishing",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Google Earth Engine</span>"
    ]
  },
  {
    "objectID": "W6_GEE.html#reflections",
    "href": "W6_GEE.html#reflections",
    "title": "6  Google Earth Engine",
    "section": "6.3 Reflections",
    "text": "6.3 Reflections\nSwitching to GEE made everything feel much more efficient. Compared to previous weeks, loading and manipulating large datasets (even for the whole world) was a breeze. The user interface of GEE was a bit overwhelming at first, but that’s probably just because it was my first time using it, but overall it seems that it offers many functionalities that I am looking forward to try in the next weeks. On that line, the GEE catalog made me realize how much data we have available and how using Remote Sensing is democratizing the access to many datasets that will definitely benefit research and data-informed policy making especially in countries where data availability is usually an issue, like my country. I think the possibilities are endless now! And on top of all that, GEE even allows us to build interactive web applications, definitely something I am looking to try as I am also taking the CASA25 module.\nOn another topic, when Ollie mentioned that even though GEE has been around for over a decade, Google could decide to shut it down at any time, it made me think that it would be such a huge loss especially after seeing how much research has been done since it was launched, but it also made me think that then it makes sense that we didn’t go straight to it(even though that’s what I was initially expecting) but instead we first focused on learning the theoretical concepts and understanding the logic behind what’s happening in the background. I think this way, even if in the future we have to change to another program, we will not be starting from scratch as we now know the foundations of it.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Google Earth Engine</span>"
    ]
  },
  {
    "objectID": "W8_Classification2.html",
    "href": "W8_Classification2.html",
    "title": "8  Classification 2",
    "section": "",
    "text": "8.1 Summary\nThis session could be divided in two main topics. On one side (and also the one i felt was easier to understand) was continuing seeing alternatives to image classification, including some preclassified datasets like MODIS or Dynamic World. The other part (which for me was a bit difficult to understand) covered key concepts in classification accuracy assessment, cross-validation techniques, the impact of spatial autocorrelation, and spatial cross validation.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Classification 2</span>"
    ]
  },
  {
    "objectID": "W8_Classification2.html#applications",
    "href": "W8_Classification2.html#applications",
    "title": "8  Classification 2",
    "section": "8.2 Applications",
    "text": "8.2 Applications\nThis week’s applications will allow me to explore the new classification techniques seen this week, focusing on OBIA. I decided to follow up on last week’s topic on informal settlements, as on Gram-Hansen et al.(2019) (last week application) as they mentioned it would be interesting to use Object-Based Image Analysis (OBIA) instead of pixel-based methods for that topic. At the same time, I wanted to also stick with land cover classification, but this time in a different environment: wetlands. This allowed me to compare OBIA’s effectiveness in two very different contexts while also seeing how these methods stack up against the more traditional classification approaches we looked at last week.\n\n8.2.1 Mapping informal settlement indicators using object-oriented analysis in the Middle East\nThis paper by Fallatah et al.(2017) also mentions how essential it is to map informal settlements for urban planing and policy decision and how traditional census surveys don’t usually cover these areas. That’s why they tried Object-Based Image Analysis (OBIA) as a tool to identify and classify informal settlements in Jeddah, Saudi Arabia, using very high resolution (VHR) satellite imagery, having as indicators vegetation extent, road network, housing patterns, and roofing materials.\nImage 2. Classification of urban settlements. In (a), the individual buildings and trees are outlined successfully in the formal areas. In the informal area in (b), buildings are generally outlined, while (c) provides an example of an informal area where deﬁning individual buildings failed due to spectral homogeneity\n\n\n\n\n\nOverall, OBIA proved useful, achieving an overall accuracy of 83% in distinguishing informal from formal areas. However, the study highlights that there’s no one size fits all approach, as when trying to replicate the model in other contexts, parameters had to be fine tuned for each case study, meaning that mapping informal settlements at scale still requires significant local expertise and manual adjustments.\n\n\n8.2.2 Mapping vegetation in the Amazonian wetlands using object-based image analysis and decision tree classification\nThis paper by De Oliveira and Rossetti (2014) tackled the issue of classifying amazonian wetlands because of their importance as unique ecosystems, but has one key challenge which is distinguishing between different types of forests and wetlands, especially in areas shaped by ancient river systems. To do so, they applied Object-Based Image Analysis (OBIA) on satellite imagery from ASTER and PALSAR, along with Digital Elevation Models (DEM), to train a Decision Tree classification model. The result was a highly detailed vegetation map with 88% accuracy, which outperformed previous studies using pixel level analysis. Using this method they were able to classify 13 different vegetation types, more than any previous Amazonian mapping project.\nOverall, OBIA showed being a great option for mapping and classifying elements that usually are mixed in irregular ways, like forests or wetlands, as by grouping similar pixels its gets better at capturing those natural patterns and reducing noise. (One note on this it’s that it would be interesting to see how much of the accuracy of the model could be attributed to OBIA, and for that a comparison with a pixel-level model would be useful)\nImage 2. Final thematic map depicting vegetation types\n\n\n\n8.2.3 Some thoughts\nLooking at these two cases side by side, a few key things stand out. First, OBIA really shines when dealing with complex landscapes, whether it’s the irregular structures of informal settlements or the patchy vegetation of wetlands, grouping similar pixels together made it better at distinguishing meaningful patterns. However, OBIA isn’t perfect, as both studies showed that it still struggles when classes have very similar spectral signatures (like bare soil vs. built up areas, or grass vs. shrubs). Another interesting takeaway is that combining multiple data sources (like optical, and DEM) improves results, like when the wetland study benefited from DEM(so, if we want better classification models we probably need to move beyond just optical imagery?). In the end, both papers also showed that classification is as much about the data you use as the method you choose. OBIA and machine learning help, but the real power comes from combining techniques, images and picking the right features to extract.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Classification 2</span>"
    ]
  },
  {
    "objectID": "W8_Classification2.html#reflections",
    "href": "W8_Classification2.html#reflections",
    "title": "8  Classification 2",
    "section": "8.3 Reflections",
    "text": "8.3 Reflections\nThis week was dense. The topics, specially the accuracy part, where kind of abstract and sometimes I couldn’t fully understand what was going on, but i got to understand the main ideas behind it. Doing the summary and reviewing the tables helped me to understand more but in general I think I’ll understand it better once I start calculating my own accuracy in exercises. Still I am not sure if I’ll get to need to do this kind of accuracy assessments at the most in-depth level once I graduate, but I think its a good idea to understand the basics so in case I encounter them in any paper or project I can understand what they mean and also not be so naive to believe in super high accuracies and always question what they really are assessing.\nRegarding the applications (and the first part of the class on other classification methods) I think it was interesting to see more available methods we have to classify images and seeing how they can be leveraged by solutions to issues that we usually encounter as urban specialists (especially the first one that I think its more related to my expertise).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Classification 2</span>"
    ]
  },
  {
    "objectID": "W7_Classification.html",
    "href": "W7_Classification.html",
    "title": "7  Classification",
    "section": "",
    "text": "7.1 Summary\nThis week we explored machine learning techniques and reviewed some remote sensing applications for them(like mapping urban sprawl, illegal logging, and land cover classification). Most of these concepts were completely new to me, so it was a great introduction to them and all the possibilities we have. Because it was my first time hearing of some of these methods, for this week’s entry I decided to structure/categorise all the methods mentioned and include what they are and how they can be useful. Also, I organized kind of a dictionary to explain some of the main concepts that were used to talk about machine learning so I can have them for future reference.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "W7_Classification.html#applications",
    "href": "W7_Classification.html#applications",
    "title": "7  Classification",
    "section": "7.2 Applications",
    "text": "7.2 Applications\nThis week, what stood out to me is how machine learning makes it possible to analyze and classify areas that might otherwise be impossible to map manually, which can be useful for governments to monitor or identify areas that require intervention but often lack resources to do so. Thinking on that, this week’s applications will be focused on examples that can help local governments monitor critical areas to improve natural resources protection and urban planning or aid distribution.\nThis week, what stood out to me is how machine learning makes it possible to analysis and classificy of areas that would be nearly impossible to map manually. This can be especially useful for governments to monitor and identify regions that require intervention but often lack the resources to do so. With that in mind, this week’s focus is on applications that can help local governments monitor critical areas and could potentially help natural resource protection, and urban planning or aid distribution.\n\n7.2.1 Detecting industrial oil palm plantations on Landsat images with Google Earth Engine\nThis paper by Huay et al.(2016) explored how GEE could be used as a low cost, accessible tool to detect industrial oil palm plantations using Landsat 8 satellite imagery. The research focused on Tripa, Indonesia, and tested different spectral bands (RGB, NIR, SWIR, etc.) to classify land cover types like forests, water, and oil palm. Different machine learning algorithms were used and Classification and Regression Trees (CART) and Random Forests, where the ones with better performance for this task. The study also found that pixel based classification has limitations, especially in distinguishing oil palm from similar land covers, and mentioned it would be useful to improve this classification in the future using object-based classification and SAR imagery.\nOverall I found this kind of research really useful as a guidance on which classification methods perform better for land use monitoring, especially useful in regions vulnerable to deforestation.\nImage 1. Classification results of Classification and Regression Trees (CART) using ALL bands (a), Random Forests (RFT) using ALL bands (b), and CART using RGB bands (c) of Landsat 8 TOA image from 2014 (d).\n\n\n\n\n\n\n\n7.2.2 Mapping Informal Settlements in Developing Countries using Machine Learning and Low Resolution Multi-spectral Data\nThis paper by Gram-Hansen et al.(2019) tackle the issue that most vulnerable people live in informal settlements but yet they often go unmapped, making aid allocations more difficult. So, they explored two approaches for detecting informal settlements using satellite imagery and different machine learning methods and resolutions. The first method relied on low-resolution Sentinel-2 data, where a pixel wise classifier is trained to recognize the spectral signature of informal settlements(the benefits being that it’s computationally efficient and uses freely available data). The second method, which is more resource intensive, applied Convolutional Neural Networks (CNN) to very high resolution (VHR) imagery, allowing for more detailed mapping.\nWhat they found is that informal settlements can be effectively detected even using only lower resolution imagery, which is a great discovery for making largescale mapping more accessible, even in settings where resources are limited.\nImage 2. Predictions of informal settlements in Kibera, Nairobi. Left: The CCF prediction of informal settlements in Kibera on low-resolution Sentinel-2 spectral imagery. Middle: Deep learning based prediction of informal settlements in Kibera, trained on VHR imagery. Right: The ground truth informal settlement mask for Kibera.\n\n\n\n\n\n\n\n7.2.3 Some thoughts\nBoth cases prove that machine learning can map complex land cover types using satellite imagery, even with freely available data like Sentinel 2 or Landsat 8, and simpler machine learning methods, making them powerful, real time and cost effective mapping tools. I think this is a great discovery for Governments which could use these rather simple models to improve their planning and response efforts.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "W7_Classification.html#reflections",
    "href": "W7_Classification.html#reflections",
    "title": "7  Classification",
    "section": "7.3 Reflections",
    "text": "7.3 Reflections\nThis week was honestly mind blowing. Since I’m not taking the CASA06 module (which goes deeper into ML), I hadn’t really explored all the different Machine Learning and classification techniques before. This felt like a great introduction to not just to the methods themselves, but also to how they can be applied to remote sensing.\nBefore this, I saw Machine Learning as something super advanced and kind of out of my league. But after this class, I realized I’ve actually been using it all along (like with something as simple as Linear Regression). It’s also funny to see all the current hype around AI and ML when in reality, a lot of these models are just statistical methods that have been around for decades. That said, I also got a sense of how many more complex approaches exist, and I hope I can explore them later in more advanced applications!\nOne key takeaway (which also ties back to something Jon mentioned in one of the first FSDS lectures) is that just because we have all these sophisticated models available doesn’t mean we should always go for the most complex one(also as the examples above show). Sometimes, a simpler model is the better choice, especially since accuracy and complexity often come at the cost of interpretability. I think this is particularly important in areas like policy and urban planning (which is what I’m aiming to work in), where having a clear, explainable model can be more valuable than one that’s just technically precise.\nOverall, this week’s content was super useful, and I’m really excited to apply it in GEE, my other modules, and even my dissertation!",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "W9_SAR.html",
    "href": "W9_SAR.html",
    "title": "9  Synthetic Aperture Radar",
    "section": "",
    "text": "9.1 Summary\nThis week, we explored SAR data which I found to be quite different from optical sensors. First of all, SAR is an active sensor (so far all of them have been passive), which means that it emits a signal (like a bat) and measures the reflected signal. Also, unlike optical imagery, this one produces black and white images, ranging only from low to high values of the amplitude of the signal reached by the sensor, making it more about texture than color.\nSimilarly to optimal imagery, it has multiple bands, each one being the result of different frequencies of microwaves. Some of these frequencies could even penetrate through thin canopies and atmospheric occlusions like clouds, but offer lower resolution. These are some of the bands:\nC band: The most useful and used SAR band (e.g. Sentinel 1). Has a good balance of resolution and penetration and generates volume scattering\nX band: Has a shorter wavelength so it’s very easy to scatter meaning it just bounces and generates a rough surface\nL band and P band: Has a longer wavelength. It can penetrate through some objects and generate double bounce scattering\nImage 1. Different bands available in SAR imagery\nSAR also has polarizations, which is the orientation of the plane in which the waves move. Meanwhile, objects have a scattering mechanism, which is the way that objects reflects the radar signal. Scattering mechanisms are a really important dimension in SAR, analogous to the color of images in optical imagery. Their types can be:\nPolarization:\n*This are the only ones available in Sentinel 1\nScattering mechanisms:\nThe types of information we get from SAR sensor are amplitude (backscatter) and phase\nAlthought SAR is a bit less useful for classification, one of its biggest strengths is change detection. Since SAR isn’t affected by cloud cover or lighting conditions, it’s more consistent and provides a reliable way to track changes in landscapes and infrastructure. A basic approach is subtracting two SAR images to highlight differences, though this doesn’t work well in areas with constant change (e.g. construction sites). Another way of approaching it was Ollie’s research on building damage detection using SAR. His method analyzed the variance in backscatter before and after a war, identifying changes outside the normal range of variation by evaluating the standard deviation of each pixel before and after war to detect changes outside of the normal noisiness/change. It was interesting to see that his statistical change detection, even though is simpler than using other Machine Learning methods, had a better performance, but makes sense as in this case of crisis scenarios you usually don’t have pretrained data that can help calibrate ML models.\nImage 2. Ollie’s research on building damage detection using SAR, showing how the mean values in SAR changed dramatically after the war started",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Synthetic Aperture Radar</span>"
    ]
  },
  {
    "objectID": "W9_SAR.html#applications",
    "href": "W9_SAR.html#applications",
    "title": "9  Synthetic Aperture Radar",
    "section": "9.2 Applications",
    "text": "9.2 Applications\nThis week I decided to review two case studies that used SAR (Synthetic Aperture Radar) for monitoring environmental conditions, particularly urban flooding and wetlands. I think these topics are really related to my interests and I could use them for future reference as I have an interest in urban resilience and environmental management, especially in areas vulnerable to climate change.\n\n9.2.1 Detection of flooded urban areas in high resolution Synthetic Aperture Radar images using double scattering\nThis paper by Mason et al.(2014) looks at urban flooding detection using SAR. It focuses on detecting flooded areas in urban environments, where traditional methods often struggle due to the interference of buildings causing shadows or cloud coverage. The study shows how double scattering between the flooded ground and adjacent buildings can be used to identify flooding, even in regions that are typically hard to detect with standard SAR. The results demonstrated that this method, particularly when combined with LiDAR data, was highly successful in spotting urban floods with great accuracy. I particularly found this one interesting because when trying my own ideas in GEE I usually struggle finding satellite images for peruvian cities with lower cloud coverage as Peru has some coastal and tropical weathers that usually make it cloudy, so maybe SAR is something I can try now.\nImage 3 (a1)Flood image (July 25, 2007) and (b1) reference image (July 22, 2008). (a2) Zoomed flood image and (b2) zoomed reference image\n\n\n\n\n\n\n\n9.2.2 Mapping and Monitoring Surface Water and Wetlands with Synthetic Aperture Radar\nThis paper by Brisco (2015) evaluates wether monitoring surface water and wetlands with SAR is an effective method. It states the importance of tracking water resources, especially in areas experiencing stress and considers SAR as a useful way to monitor changes in surface water levels and wetlands. By using SAR data in combination with land cover maps and DEM (Digital Elevation Models). The results of this study shows that SAR in combination with other images (forming a multisource image) can help track seasonal or annual changes in wetlands, such as flooding or vegetation changes, more effectively than using only optical imagery and are a state-of-the-art option. But also mentions some limitations or things to consider like how important it is to take care of different scales and resolutions involved in multisource images.\nImage 4 Change in double-bounce scattering between June 6 and August 17, 2008, in Dongting Lake, China and the surrounding area. This is due to the change in SAR scattering mechanisms as the water level drops after snowmelt runoff ceases and the vegetation is no longer flooded.\n\n\n\n9.2.3 Some thoughts\nOverall, both studies highlight how versatile SAR is when it comes to environmental monitoring, whether it’s for detecting urban flooding or tracking changes in wetlands. Also, they both mention the reliability on SAR’s ability to provide consistent, high-resolution data, even in challenging conditions (like bad weather). Both approaches, in general, show how SAR can enhance monitoring and management of water related issues, but I would like to see which other case studios would also benefit from this imagery.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Synthetic Aperture Radar</span>"
    ]
  },
  {
    "objectID": "W9_SAR.html#reflections",
    "href": "W9_SAR.html#reflections",
    "title": "9  Synthetic Aperture Radar",
    "section": "9.3 Reflections",
    "text": "9.3 Reflections\nThis week was super interesting because SAR introduces a completely different way of analyzing Earth’s surface compared to optical imagery. Before coming to UCL, I hadn’t even heard of SAR, so it’s exciting to dive into something new and realize how useful it is. Also, although I reviewed some SAR applications, both of them are related to water bodies, and I think it would be interesting to see what other type of applications have already used SAR data.\nOne thing that stood out(also aligned with one of my reflections weeks ago) was how more complex models aren’t always better. Ollie’s study showed that deep learning, despite all the AI hype, wasn’t the best option for detecting building damage and that his statistical approach was actually more reliable. This made me think that the best option its actually to really understand what are all the possibilities each option bring us and evaluate which makes more sense for each case depending on the available data and expected outputs, and that at the end, choosing the right tool is more important than just picking the most advanced one.\nIt was also great to try SAR in GEE and see how all these concepts translate into real world analysis. I quite liked the focus Ollie has on his practicals, using it to assess serious urban and social issues which makes me think that what we are learning can have a greater impact on society(something I have been trying to incorporate when deciding what to do for my dissertation as I am currently also about to present my dissertation proposal).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Synthetic Aperture Radar</span>"
    ]
  },
  {
    "objectID": "W6_GEE.html#summary",
    "href": "W6_GEE.html#summary",
    "title": "6  Google Earth Engine",
    "section": "",
    "text": "GEE Elements\nMeaning\n\n\n\n\nImage\nRaster\n\n\nFeature\nVector\n\n\nImageCollection\nA stack of images\n\n\nFeatureCollection\nA stack of features\n\n\nClient side\nFront end: The part that users interact with\n\n\nServer side\nBack end: Where the data is retrieved and processed\n\n\n\n\n\n\nScale & Resolution: GEE aggregates data based on zoom level—so the further you zoom out, the bigger each pixel represents. Scale and resolution are always linked!\nProjection: Everything is converted to the Mercator projection by default\nFiltering: To avoid loading excessive data, we always filter by time and spatial bounds\nOperations & Applications: GEE lets us perform geometric operations, corrections, enhancements, and even advanced tasks like machine learning, classification, and deep learning\n\n\n6.1.1 Practical\nIn the practical, we applied these concepts to the city of Delhi, but I also decided to analyze a Peruvian city following the same steps. It was interesting to apply the techniques in a familiar context, though I encountered limitations, such as needing to increase the cloud presence threshold to obtain enough images. But overall it was amazing to finally use GEE and see how fast and practical it was to load the data for anywhere in the world and run the analysis.\nImage 2: PCA analysis of Iquitos, Peru",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Google Earth Engine</span>"
    ]
  },
  {
    "objectID": "W3_RemoteSensingData.html#summary",
    "href": "W3_RemoteSensingData.html#summary",
    "title": "4  Remote Sensing Data",
    "section": "",
    "text": "4.1.1 Key terms\nSo far many words have been repeated multiple times during lectures and papers I have reviewed and sometimes I still mix them up, so here they are summarised:\n\n\n\n\n\n\n\nTerm\nMeaning\n\n\n\n\nIrradiance\nRadiation reaching Earth from the Sun\n\n\nRadiance\nRadiation leaving Earth toward the satellite\n\n\nReflectance\nThe proportion of incoming radiation reflected by a surface\n\n\nSurface Reflectance\nReflectance at the bottom of the atmosphere (after atmospheric correction)\n\n\nAnalysis-ready data\nPreprocessed images, usually in Surface Reflectance format (so we don’t have to do all the corrections ourselves!)\n\n\n\nImage 2. Illustration done to summarise the main terms used in Remote Sensing\n\n\n\n\n\n\n\n4.1.2 Corrections:\nThis week we saw some of the corrections that raw sensor data could use before we start manipulating it. Most of them are already applied to images we use, as we usually use Analysis-ready data, but I think knowing what kind of preprocessing images have is useful at least as a general knowledge or if we were to work on raw data(maybe needed if I were to work with drone imagery?).\n\n\n\n\n\n\n\nTerm\nMeaning\n\n\n\n\nGeometric correction\nAligns misaligned images using control points and regression (basically shifting everything so it matches properly). Useful for correcting historical maps to match modern spatial data.\n\n\nAtmospheric correction\nRemoves distortions caused by atmospheric scattering (because the atmosphere its always there!)\n\n\nDark Object Subtraction (DOS)\nMethod for atmospheric correction that finds something that should have zero reflectance and subtracts its measured value from all pixels\n\n\nPseudo-Invariant Features (PIFs)\nMethod for atmospheric correction that uses objects with stable reflectance over time as references for correction (great for long-term studies)\n\n\nFlat correction\nMethod for atmospheric correction that uses field-measured values (without atmospheric interference) as a baseline for regression\n\n\nOrthorectification\nCorrection that fixes distortions from sensor tilt\n\n\nRadiometric calibration\nConverts Digital Number (DN) to spectral radiance to standardize measurements\n\n\n\n\n\n4.1.3 Enhancements:\nI would say enhancements are a way of transforming images using data that is already available in the image itself, to better show some features. Some of the methods are:\n\n\n\n\n\n\n\nTerm\nMeaning\n\n\n\n\nContrast adjustments\nModifies image appearance without changing data values (basically, just making it look better)\n\n\nBand ratios\nDividing one band by another (e.g., NDVI for vegetation analysis)\n\n\nFiltering\nCan be low-pass (smoothing) or high-pass (highlighting edges, like for detecting buildings)\n\n\nPrincipal Component Analysis (PCA)\nReduces dimensionality to capture the most variance (helps focus on key changes in images, but could loose some interpretability)\n\n\nTexture analysis\nMeasures similarity between a pixel and its neighbors (useful for spotting urban areas or specific land features)\n\n\nImage fusion & pansharpening\nMerges data from different sensors, often using high-resolution bands to sharpen lower-resolution images",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Remote Sensing Data</span>"
    ]
  },
  {
    "objectID": "W7_Classification.html#summary",
    "href": "W7_Classification.html#summary",
    "title": "7  Classification",
    "section": "",
    "text": "7.1.1 Some foundational concepts:\nHuman learning: uses inductive learning, so given our experience of the world, we make inferences on the images or data we see\nExpert system: system that uses human knowledge as a base to solve problems. Tries to show a computer how humans reach decisions\nMachine learning: science of computer modeling of learning process trying to replicate inductive learning\nTwo schools: on one side, Traditional classifiers, they don’t apply a model, they just divide the data based on a feature space. On the other side, the newer methods use Machine Learning.\n\n\n7.1.2 Classification methods\nImage 1. Summary of all classification methods mentioned in class\n\nLinear Regression: Predicts continuous values by finding the best fit between independent and dependent variables\nDecision Trees (CART): Useful when linear regression assumptions don’t hold. Classification trees assign discrete categories, while regression trees predict continuous values by recursively splitting the data\nRandom Forests: Multiple decision trees working together, increasing accuracy but reducing interpretability. The model votes on the most likely classification, which makes it more reliable but harder to understand\nClustering/K-means: Unsupervised method, similar to DBScan, it makes clusters depending on similarities with other pixels. Useful when categories are unknown\nISODATA: Extension of K-means. Similar but has some inputs called hyperparameters\nMaximum likelihood, density slicing, parallelpiped : They are more classical methods not used that much nowadays. They are supervised methods that start with a class definition, selects the training data, and applies the model to the rest of the data\nSupport Vector Machine SVM: Comparable to linear regression. Instead of a line there’s a hyperplane and many support vectors . This hyperplane separated two classes but allows some misclassificationndefined(soft margin). It’s a two class comparison but could be replicated for multiple classes\nNeural Networks & Deep Learning: We didn’t cover these in detail, but they represent the next level, more powerful but often a “black box,” meaning high accuracy at the cost of interpretability\n\n\n7.1.3 Machine learning concepts:\nOverfitting: When the model fits the training data too well that makes it useless for new data. In order to avoid it we have to check for low bias (difference between predicted value and true value) and low variance (difference in accuracy between train fit and test fit)\nImage 2. Bias and variance in overfitting\n\nSource\nTrain and test data: we develop the model with train data and it doesn’t see the test data. Then I use that model to predict other pixels including validating(test) pixels. With this comparison i can assess the accuracy(which we will learn more next week)\nCross-validation: same process but changing what is your train and test data each iteration. It ensures a more robust model.\nBootstrap sampling: sampling by replacement. it means some rows of data can be duplicated\nOut of bag sample: rows of data not used for the random forest. It is used like a validation dataset and is useful to get the out of bag error\nSupervised classification: The model learns from labeled training data to classify new data.\nUnsupervised classification: The model classifies data given a method or algorithm. It has no human input and categories are not known a priori\nHyperparameters: control variables\nBlack box: you just know if the model is good but loose interpretability. The most accurate models usually have less interpretability.\nSpatial autocorrelation: if the train and test data are too close spatially, they are probably very similar and the accuracy could be too high but the model not robust enough (overfitting)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "W9_SAR.html#summary",
    "href": "W9_SAR.html#summary",
    "title": "9  Synthetic Aperture Radar",
    "section": "",
    "text": "VV (Vertical-Vertical): Sensitive to rough surfaces and water bodies. *\nVH (Vertical-Horizontal): Detects volume scattering from vegetation. *\nHH (Horizontal-Horizontal): Picks up double-bounce reflections, often seen in urban areas.\n\n\n\n\nRough surface scattering – Common in open terrain, chaotic reflections. Very visible in VV polarization\nVolume scattering – Occurs in vegetation or complex surfaces. Mostly sensitive in VH polarization\nDouble bounce scattering – Happens with vertical structures like buildings. Mostly sensitive in HH polarization\n\n\n\nAmplitude (Backscatter): Measures the “loudness” of the returned signal, useful for texture analysis\nPhase: Measures when is the reflected signal received. It allows us to measure distance to the ground and change detection over time, but it’s not available in Google Earth Engine (GEE)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Synthetic Aperture Radar</span>"
    ]
  },
  {
    "objectID": "W8_Classification2.html#summary",
    "href": "W8_Classification2.html#summary",
    "title": "8  Classification 2",
    "section": "",
    "text": "8.1.1 Alternatives to Image Classification\nDynamic World: open-access semisupervised model with already preprocessed data to obtain info on tree cover, built index, etc, that is continuously being updated. Something interesting it that it uses Convolutional Neural Networks, which is a form of deep learning that uses a moving window for classification. Some cons would be that its not too detailed, has some inaccuracies because of using TOA reflectance and lacks interpretability.\nOBIA: Object based image analysis. Instead of classifying cells, segments images in meaningful shape based on similarity, called superpixels and then classifies them. It uses SLIC which is a method to refine classification that checks for homogeneity of colors and closeness to center\nSubpixel analysis: recognizes that one pixel might include multiple land cover types so we could calculate the proportion of each pixel corresponding to each class. It’s computed using matrix inversion techniques, with each fraction summing to one\n\n\n8.1.2 Accuracy assessment\nThen moving to accuracy assessment. First of all I found this table to be the basis of all next concepts:\n\n\n\n\n\nSource\nSo, there are many indexes to measure accuracy and one consideration to have present its that it’s not possible to maximize all accuracy measures at the same time. Some of them are:\n\nPA - Producer accuracy : true positive and false negative rate\nUA - User accuracy: true positive and false positive rate\nOA- Overall accuracy: true positive and true negative compared to all classifications\nErrors of omission: 100 - PA\nErrors of Comission: 100 - UA\nKappa coefficient: accuracy of an image compared to the results by chance, but its use is controversial\nF1 measure: combines PA and UA but doesn’t take in account true negatives\n\nHow do we get test data for accuracy assessment?\nModels need to be tested on different datasets to ensure they generalize well. Tipically basic validation splits the data into training and testing subsets but there are other ways to improve reliability like using Crossvalidation or leave-one-out crossvalidation (this one is not too used in remote sensing)\nWe also saw Spatial Autocorrelation and things to consider about it:\nSpatial autocorrelation means that data points close to each other are more similar than those farther apart (Tobler’s Law: “everything is related, but near things are more related”). If training and test data are too close, the model sees a “sneak preview” of test data, leading to an overestimated accuracy. Traditional pixel based accuracy assessments often suffer from this issue.\nSpatial cross validation\nUnlike standard crossvalidation, spatial crossvalidation ensures that training and test data are spatially separated. This reduces the effect of spatial autocorrelation and provides a more realistic accuracy estimate. Some methods include:\n\nObject-based classification: Instead of individual pixels, entire objects (e.g. buildings or vegetation patches) are classified (like OBIO?)\nSpatial partitioning: Training and test data are assigned to separate geographic areas to avoid overlap.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Classification 2</span>"
    ]
  }
]