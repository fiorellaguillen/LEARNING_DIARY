[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CASA0023 Learning Diary",
    "section": "",
    "text": "Introduction\nHello world!\nThis Learning Diary is part of the CASA0023 module, Remotely Sensing Cities and Environments, which I’m taking as part of the MSc Urban Spatial Science at UCL. Here, you’ll find weekly entries summarizing key takeaways from each lecture, examples of practical applications, and my personal reflections.\nBut first, let me introduce myself!\nMy name is Fiorella, and I’m currently pursuing an MSc in Urban Spatial Science. Back in 2021, I graduated as an architect and urbanist in Peru. Since then, I have developed a strong interest in sustainability, climate change, water resources, and urban settlements, particularly how these factors affect urban and natural environments. As an example, during my undergraduate thesis, I focused on assessing changes in a river in the Peruvian Amazon, accelerated by climate change, and proposed solutions to the urban challenges that emerged from these changes.\nSince graduating, I have worked in the public sector, contributing to Lima’s Rimac River renaturalization project. This role involved urban analysis, master planning, urban design, and policy implementation, with a strong emphasis on risk prevention, and nature-based solutions.\nWhile I have some basic experience using Earth Observation (EO) and remote sensing to address topics like those mentioned above, my approach has been largely empirical. That’s why I find this module to be an exciting opportunity to deepen my knowledge of these tools and processes, enabling me to better inform policies and urban solutions in future projects or research.\nI am particularly interested in how these technologies can be applied to urban challenges in the Global South, where they are often underutilized due to knowledge gaps. That’s why, throughout this diary, my reflections will aim to connect these tools to my background and interests, as well as explore their real-world applications in contexts similar to my country’s.\nAnd if you are curious about my previous projects, here you can have a glimpse at them!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Welcome</span>"
    ]
  },
  {
    "objectID": "W1_Introduction.html",
    "href": "W1_Introduction.html",
    "title": "2  An Introduction to Remote Sensing",
    "section": "",
    "text": "2.1 Summary\nThis week’s lecture covered the basics of Earth Observation (EO) and Remote Sensing including topics such as definitions, relevance and terminology of Earth Observation, as well as more technical concepts to describe sensors (resolutions, types, interactions of light). In this summary, I’ll focus on the latter, mainly defining some of the key characteristics of sensors (Image 1). To have a clearer understanding of these concepts, I will exemplify them with information from two of the main satellites discussed in class: Landsat and Sentinel (Image 2).\nImage 1: Mind map of main characteristics of sensors\nImage 2: Mind map comparing main characteristics of two of the main sensors: Landsat and Sentinel\nFrom this comparison, it’s worth noticing that overall, Sentinel shows better characteristics, like spatial resolutions with more detail, more spectral bands and more frequent temporal resolution than Landsat. It would be interesting to see in which cases, despite having seemingly lower characteristics, Landsat imagery could represent a better tool to assess an spatial problem.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>An Introduction to Remote Sensing</span>"
    ]
  },
  {
    "objectID": "W2_Portfolio.html",
    "href": "W2_Portfolio.html",
    "title": "3  Portfolio tools: Xaringan and Quarto",
    "section": "",
    "text": "This week we explored how tools like Xaringan and Quarto can be used to have reproducible presentations and documents, useful for data science projects. As an exercise, I have developed this book using Quarto and a presentation on the Sentinel -2 satellite using Xaringan, which you can see here:\n\n\n\n\n\n\n\n\nAs a reflection on this exercise, I found that learning how to use these tools, specially Xaringan, was quite challenging, and probably not the best option for a simple presentation, but could be really useful when used to display code or data through tables for example, or when using some of the interactive tools that XaringanExtra has.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Portfolio tools: Xaringan and Quarto</span>"
    ]
  },
  {
    "objectID": "W1_Introduction.html#summary",
    "href": "W1_Introduction.html#summary",
    "title": "2  An Introduction to Remote Sensing",
    "section": "3.1 Summary",
    "text": "3.1 Summary\nThis week’s lecture focused on the foundations of Earth Observation (EO) and Remote Sensing. From all the contents of the class, I will focus, on this summary, on the differences between two main sensors: Landsat and Sentinel. This comparison will draw on some of the contents of the lecture, like types of sensors, resolutions, spectral bands, and applications.\n*Pending to upload mind map",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>An Introduction to Remote Sensing</span>"
    ]
  },
  {
    "objectID": "W1_Introduction.html#applications",
    "href": "W1_Introduction.html#applications",
    "title": "2  An Introduction to Remote Sensing",
    "section": "2.2 Applications",
    "text": "2.2 Applications\nSince this week’s summary included a comparison between Landsat and Sentinel sensors, I considered relevant to read a paper that compares both satellites’ imagery applied to analyse Burn Severity Mapping in Western North America. This paper by Howe et al (2022) analysed 26 fires in western North America, and found that “Sentinel generally performed as well or better than Landsat for four spectral indices of burn severity”, also that Sentinel’s finer spatial resolution helps to identify fine-scale fire effects and therefore has a more precise identification of spatial patterns of fires and burned areas.\nOn the other hand, the paper “Analysis of urban heat islands with Landsat satellite images and GIS in Kuala Lumpur Metropolitan City” (Kasniza et al., 2023) explored the evolution of heat islands and their relationship to land surface temperature in Kuala Lumpur using Landsat 8 imagery. Instead of focusing on the research details, I want to focus on the methodology, where they explained that Landsat 8 was chosen because it provides thermal data through Band 10: Thermal Infrared (which isn’t available on Sentinel). This band was used alongside other bands like NDVI and NIR for the analysis.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>An Introduction to Remote Sensing</span>"
    ]
  },
  {
    "objectID": "W1_Introduction.html#reflections",
    "href": "W1_Introduction.html#reflections",
    "title": "2  An Introduction to Remote Sensing",
    "section": "2.3 Reflections",
    "text": "2.3 Reflections\nAfter reviewing the main concepts of sensors and comparing Landsat and Sentinel satellites, it seemed that Sentinel has more advantages in terms of its various resolutions and some uses like burn mapping. However, after reviewing literature and considering the pros and cons of each, it’s worth pointing out that Landsat is still highly relevant, especially to certain topics, for example when it comes to analyzing heat, thanks to its thermal bands. Also, since Sentinel is relatively new, if I were looking to analyze long-term trends, Landsat would probably be the best option, which is something to keep in mind for future projects. In general, the main reflection from this, would be that depending of the research we want to do, it’s worth assessing which sensor would give us greater information, and looking for examples in previous research is a great tool for that.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>An Introduction to Remote Sensing</span>"
    ]
  },
  {
    "objectID": "W3_RemoteSensingData.html",
    "href": "W3_RemoteSensingData.html",
    "title": "4  Remote Sensing Data",
    "section": "",
    "text": "4.1 Summary\nThis week introduced a lot of new concepts related to corrections, data joining, and enhancements in remote sensing. To keep things clear for future reference, I organized them into categories and decided to do kind of a “dictionary” of these concepts, so it will be useful to understand them when reviewing literature and seeing applications of it.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Remote Sensing Data</span>"
    ]
  },
  {
    "objectID": "W3_RemoteSensingData.html#applications",
    "href": "W3_RemoteSensingData.html#applications",
    "title": "4  Remote Sensing Data",
    "section": "4.2 Applications",
    "text": "4.2 Applications\nThese corrections and enhancements are essential for making sure remote sensing data is accurate and actually useful. For example, atmospheric corrections are super important for environmental monitoring, where we need consistent multi-temporal comparisons. Geometric corrections are crucial for historical imagery alignment, especially in change detection studies. Radiometric calibration ensures that sensor data is comparable across time and space.\nEnhancements also have a ton of applications in urban studies, agriculture, and disaster response. Band ratios (like NDVI) help in assessing vegetation health, while texture analysis is great for land-use classification. PCA is often used for detecting land-cover changes, highlighting major differences in multi-temporal images. Image fusion techniques improve spatial resolution, making high-precision mapping and infrastructure monitoring more effective.\nOne interesting example is multi-sensor fusion, where high-resolution panchromatic images enhance low-resolution multispectral data to improve feature detection. A study titled Multi-Sensor Fusion: A Simulation Approach to Pansharpening Aerial and Satellite Images (MDPI, 2020) discusses different ways to sharpen images and extract better features using fusion techniques.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Remote Sensing Data</span>"
    ]
  },
  {
    "objectID": "W3_RemoteSensingData.html#reflections",
    "href": "W3_RemoteSensingData.html#reflections",
    "title": "4  Remote Sensing Data",
    "section": "4.3 Reflections",
    "text": "4.3 Reflections\nAt first, learning about corrections and enhancements felt unnecessary as most modern datasets come preprocessed anyway. But after seeing the applications I think its worth knowing the possibilities we have and what we could do if we ever encounter raw data(maybe not even satellite data). Even if we end up using only Analysis-ready data, I now think it’s important to know what kind of process it has been through so we can assess it better and know what we are working with. Even though I now realise its importance, I found this week to be a bit overwhelming because of all the concepts, but after organizing everything, it started making sense. Some papers are still hard to follow with all their complex formulas, but at least now I have a better foundation to understand what’s going on, so I guess we are having progress!\nA couple things catched my attention this week. One thing that surprised me was how much regression is used in remote sensing. I always thought of regression as something for statistical modeling (like in CASA007), but it’s actually everywhere here, aligning images, calibrating radiance, enhancing quality, etc. The other one was seeing Andy’s fieldwork for atmospheric correction which seems pretty cool. It made me think that sometimes we might need higher levels of precision for our images and its interesting to see all the options we have available to handle that.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Remote Sensing Data</span>"
    ]
  },
  {
    "objectID": "W4_Policy.html",
    "href": "W4_Policy.html",
    "title": "5  An Introduction to Remote Sensing",
    "section": "",
    "text": "5.1 Summary\nThe Plan Integral de Reconstrucción con Cambios (PIRCC) is a strategy developed by the peruvian government to rebuild and strengthen areas affected by El Niño Costero. Its goal is not just to repair damaged infrastructure but also to make communities more resilient against future disasters. The plan promotes sustainable urban development and better risk management, recognizing that prevention is more effective than post-disaster reconstruction. Also, given the increasing impact of climate change, the PIRCC highlights the need to prepare for extreme weather events rather than just reacting to them.\nThat said, while the plan includes prevention measures, these have mostly focused on building flood defenses, dikes, and urban development plans for affected cities, but is missing a set of tools to better inform the location of these projects from a data-driven approach, and that overall, allow long-term risk monitoring, including tools that could provide real-time data and analysis to guide urban planning and infrastructure decisions. Without that, rebuilding efforts might not be too strategic, and could have less accuracy, reducing that way its effectiveness to prevent future natural phenomena.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>An Introduction to Remote Sensing</span>"
    ]
  },
  {
    "objectID": "W4_Policy.html#applications",
    "href": "W4_Policy.html#applications",
    "title": "5  An Introduction to Remote Sensing",
    "section": "5.2 Applications",
    "text": "5.2 Applications\nConsidering this, and in order to assist with contributing to PIRCC’s policies goals, I consider it would be relevant to create a monitoring tool using Landsat imagery(because of its long-term data) that can analyse previous years’ affected areas by El Niño and with that information classify land risk. These data can be used to identify regions with repeated flooding, land degradation, and other vulnerability factors that are indicative of high-risk areas.\nFor that, spectral indices such as NDVI (Normalized Difference Vegetation Index) and NDBI (Normalized Difference Built-up Index) could be used to assess land cover changes and urban expansion, helping to identify areas where improvements are most needed.\nWith this application, we could shift towards a more data-driven approach to disaster prevention that could be replicated in different areas affected by this phenomenon, and ultimately, helps better inform policies to maximize its effects.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>An Introduction to Remote Sensing</span>"
    ]
  },
  {
    "objectID": "W4_Policy.html#reflections",
    "href": "W4_Policy.html#reflections",
    "title": "5  An Introduction to Remote Sensing",
    "section": "5.3 Reflections",
    "text": "5.3 Reflections\nThrough this brief, I could understand how to link what we are learning on Earth Observation, to planning urban development, which is related to my background as an urban planner. Also, having a critical analysis of the policy, it made me realize that they are lacking some important data that could improve its effectiveness.\nThrough the application, I consider satellite imagery can be leveraged to address specific challenges posed by El Niño, and can better anticipate future risks, making the PIRCC more effective and sustainable in the long term.\nAdditionally, through this exercise, I’ve recognized the importance of collaboration between different data sources, such as satellite imagery and ground-level assessments, to create a comprehensive view of the situation. Also, it’s clear that while remote sensing can provide invaluable insights, it must be combined with local knowledge and community engagement to ensure that the data is used effectively in shaping policies and practices on the ground.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>An Introduction to Remote Sensing</span>"
    ]
  },
  {
    "objectID": "W6_GEE.html",
    "href": "W6_GEE.html",
    "title": "6  Google Earth Engine",
    "section": "",
    "text": "6.1 Summary\nThis week, we finally started using Google Earth Engine (GEE), and honestly, it felt like a game changer compared to earlier tools like SNAP. GEE makes satellite image analysis way more practical and less time-consuming. We went through the basics: how to access and manipulate satellite imagery, filter and scale values, join images, clip them, and even perform Principal Component Analysis (PCA).\nOne thing to mention is that GEE runs on JavaScript, which sounds intimidating at first, but so far, we’ve only used relatively simple commands, so it’s not that bad. I have summarized some of the main things to remember when using Javascript in GEE, in the following image:\nAlso, some words to keep in mind when working in GEE have been summarized here:\nA key takeaway is that using GEE’s built-in server side functions is way better performance wise. So, for example, instead of writing traditional loops, we should use functions like .map() to speed things up as they have been optimized for GEE.\nOther important concepts:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Google Earth Engine</span>"
    ]
  },
  {
    "objectID": "W6_GEE.html#applications",
    "href": "W6_GEE.html#applications",
    "title": "6  Google Earth Engine",
    "section": "6.2 Applications",
    "text": "6.2 Applications\nGEE’s massive satellite data catalog opens the door for a huge range of applications. We can do everything from climate studies to nighttime light analysis, with global coverage for most datasets. This is especially valuable for regions in the Global South, where detailed geospatial datasets are often lacking. With GEE, we can bypass that limitation and still conduct meaningful research and analysis.\nSome key application areas:\n\nEnvironmental Monitoring: Analyzing land cover changes, deforestation, and water body dynamics.\nUrban Studies: Monitoring urban expansion, heat islands, and transportation networks.\nAgriculture & Vegetation: Using NDVI and other indices to assess crop health and vegetation patterns.\nDisaster Response: Mapping flood-prone areas, tracking wildfires, and assessing damage after natural disasters.\nAir Quality & Climate: Analyzing pollution patterns and long-term climate trends.\n\nThere’s also the possibility of integrating GEE with machine learning models for tasks like land classification and object detection. And on top of all that, GEE even allows us to build interactive web applications, definitely something I am looking to try as I am also taking the CASA25 module.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Google Earth Engine</span>"
    ]
  },
  {
    "objectID": "W6_GEE.html#reflections",
    "href": "W6_GEE.html#reflections",
    "title": "6  Google Earth Engine",
    "section": "6.3 Reflections",
    "text": "6.3 Reflections\nSwitching to GEE made everything feel much more efficient. Compared to previous weeks, loading and manipulating large datasets (even for the whole world) was a breeze. The user interface of GEE was a bit overwhelming at first, but that’s probably just because it was my first time using it, but overall it seems that it offers many functionalities that I am looking forward to try in the next weeks. On that line, the GEE catalog made me realize how much data we have available and how using Remote Sensing is democratizing the access to many datasets that will definitely benefit research and data-informed policy making especially in countries where data availability is usually an issue, like my country. I think the possibilities are endless now!\nOn another topic, when Ollie mentioned that even though GEE has been around for over a decade, Google could decide to shut it down at any time, it made me think that it would be such a huge loss especially after seeing how much research has been done since it was launched, but it also made me think that then it makes sense that we didn’t go straight to it(even though that’s what I was initially expecting) but instead we first focused on learning the theoretical concepts and understanding the logic behind what’s happening in the background. I think this way, even if in the future we have to change to another program, we will not be starting from scratch as we now know the foundations of it.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Google Earth Engine</span>"
    ]
  },
  {
    "objectID": "W8_Classification2.html",
    "href": "W8_Classification2.html",
    "title": "8  An Introduction to Remote Sensing",
    "section": "",
    "text": "8.1 Summary",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>An Introduction to Remote Sensing</span>"
    ]
  },
  {
    "objectID": "W8_Classification2.html#applications",
    "href": "W8_Classification2.html#applications",
    "title": "8  An Introduction to Remote Sensing",
    "section": "8.2 Applications",
    "text": "8.2 Applications",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>An Introduction to Remote Sensing</span>"
    ]
  },
  {
    "objectID": "W8_Classification2.html#reflections",
    "href": "W8_Classification2.html#reflections",
    "title": "8  An Introduction to Remote Sensing",
    "section": "8.3 Reflections",
    "text": "8.3 Reflections",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>An Introduction to Remote Sensing</span>"
    ]
  },
  {
    "objectID": "W7_Classification.html",
    "href": "W7_Classification.html",
    "title": "7  Classification",
    "section": "",
    "text": "7.1 Summary\nThis week we explored machine learning techniques and reviewed some remote sensing applications for them(like mapping urban sprawl, illegal logging, and land cover classification). Most of these concepts were completely new to me, so it was a great introduction to them and all the possibilities we have. Because it was my first time hearing of some of these methods, for this week’s entry I decided to structure/categorise all the methods mentioned and include what they are and how they can be useful. Also, I organized kind of a dictionary to explain some of the main concepts that were used to talk about machine learning so I can have them for future reference.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "W7_Classification.html#applications",
    "href": "W7_Classification.html#applications",
    "title": "7  Classification",
    "section": "7.2 Applications",
    "text": "7.2 Applications\nMachine learning is a game-changer for remote sensing, particularly in regions where traditional data collection is difficult or expensive. Some key applications include\nUrban Growth & Land Cover Classification: Identifying informal settlements, urban expansion, or deforestation using satellite imagery. Supervised classification methods like SVM or Random Forests are commonly used for this.\nDisaster Response & Environmental Monitoring: Mapping flooded areas or monitoring forest loss. Unsupervised methods like K-means can be used to detect changes over time.\nAgriculture & Food Security: Classifying different crop types, predicting yields, or assessing drought impacts. Machine learning models can process multispectral images to detect vegetation health.\nClimate Change Studies: Analyzing glaciers, urban heat islands, or desertification trends using historical and real-time satellite data.\nData-Sparse Regions: In Global South countries, official land-use maps may be outdated or nonexistent. Machine learning allows researchers to generate high-quality maps from remote sensing data, filling gaps in public datasets.\nWhat stood out to me is how machine learning makes it possible to analyze and classify areas that might otherwise be impossible to map manually. In cities with rapid urbanization but little official data, these techniques can provide real-time insights that governments and researchers can act upon.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "W7_Classification.html#reflections",
    "href": "W7_Classification.html#reflections",
    "title": "7  Classification",
    "section": "7.3 Reflections",
    "text": "7.3 Reflections\nThis week was honestly mind blowing. Since I’m not taking the CASA06 module (which goes deeper into ML), I hadn’t really explored all the different Machine Learning and classification techniques before. This felt like a great introduction to not just to the methods themselves, but also to how they can be applied to remote sensing.\nBefore this, I saw Machine Learning as something super advanced and kind of out of my league. But after this class, I realized I’ve actually been using it all along (like with something as simple as Linear Regression). It’s also funny to see all the current hype around AI and ML when in reality, a lot of these models are just statistical methods that have been around for decades. That said, I also got a sense of how many more complex approaches exist, and I hope I can explore them later in more advanced applications!\nOne key takeaway (which also ties back to something Jon mentioned in one of the first FSDS lectures) is that just because we have all these sophisticated models available doesn’t mean we should always go for the most complex one. Sometimes, a simpler model is the better choice, especially since accuracy and complexity often come at the cost of interpretability. This is particularly important in areas like policy and urban planning (which is what I’m aiming to work in), where having a clear, explainable model can be more valuable than one that’s just technically precise.\nOverall, this week’s content was super useful, and I’m really excited to apply it in GEE, my other modules, and even my dissertation!",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "W9_SAR.html",
    "href": "W9_SAR.html",
    "title": "9  An Introduction to Remote Sensing",
    "section": "",
    "text": "9.1 Summary\nSAR is an active sensor. It is kind of like a bat. It emits a chirp and listens to the reflected signal\nCompared to optical images, this one is B&W because it only goes from low to high values of the amplitude of the signal that was emmited by the sensor that has been reflected and reached by the sensor. its the only information giving us, textural not spectral\nIt has multiple bands, each one being the result of different frequencies of microwaves. Some are longer(that penentrate through thin canopies and atmospheric occlusions like clouds, but lower resolution)\nC band is the most useful SAR band, the most used in active remote sensing (e.g. Sentinel 1). Short enough that the resolution is good but Long enough that is not super sensitive to interferences\nSAR is seeing at 45° and its in orbit, its moving.\nSintetic aperture: synthetic antenna. the motion of the sensor is used to create a synthetic anthena\nintead of having a huge antenaa(which means higher resolution), it receives images as moving, gaining distance and resolution\nSAR has different polarizations: orientations of the plane in which the waves move.\nObjects have scattering mechanism: the way that objects reflects the radar signal\nScattering mechanisms are a really important dimension in SAR, analogous to the color of images in optical imagery\nTypes:\nrough surface scattering: chaotic mechanism. Signal goes in many directions. Very visible in VV polarization\nVolume scattering: middle ground, for middle sized objects. Band called VH\nDouble bounce scattering: generated by buildings or vertical structures. Mostly sensitive in HH polarization\nwe only have access to VH and VV in Sentinal 1\nX band is very easy to scatter, it just bounces and generates a rough surface C band penetrates leaves and shows branchs, generates volume scattering. L band and P band can penetrate through branches and generate double bounce scattering\nThe types of information we get from SAR sensor are amplitude or backscatter and phase\nAmplitude: the loudness of the signal\nPhase: at what point in the wave cycle we receive the reflected signal back to us. allows us to see distance to the ground and changes over time of it.(not available in GEE)\nSAR Imagery in GEE is preprocessed, corrected for terrain.\nSAR is a bit less useful for classification, but useful to identify different changes. Useful for change detections as its more consistent than optical imagery.\nTo identify changes we can substact two images from each other and see the changes, but if its an area in constant change like stations or construction sites, its not useful.\nTo identify anomalous changes, Ollie has a paper using SAR to analyse building damage detection. It uses info on variance, evaluates the standar deviation of each pixel before and after the war and shows the difference in the average backscatter outside of the bounds of the normal variation. So identifies changes anomalous, out of the normal pattern of noisiness/change.\nHe also finds that is useful to disaggregate orbital trajectories (ascending and discending) to ensure consistency, as well as different polarisations by a t-test of each one.\nThis option outperforms deep learning on optical imagery as deep learning is not as accuracte on not trained data, which isnt available when analysing war effects. Instead this statistical change detection methos is better, more accurate for each case without trained data,",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>An Introduction to Remote Sensing</span>"
    ]
  },
  {
    "objectID": "W9_SAR.html#applications",
    "href": "W9_SAR.html#applications",
    "title": "9  An Introduction to Remote Sensing",
    "section": "9.2 Applications",
    "text": "9.2 Applications",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>An Introduction to Remote Sensing</span>"
    ]
  },
  {
    "objectID": "W9_SAR.html#reflections",
    "href": "W9_SAR.html#reflections",
    "title": "9  An Introduction to Remote Sensing",
    "section": "9.3 Reflections",
    "text": "9.3 Reflections\nThis week was also cool. We saw SAR and it was a whole differente range of options that optimal imagery doesn have. Also it was interesting to see the application that Ollie showed as it alligns with what i reflected on some weeks ago, when i was discussing about the fact that sometimes more advanced methods , in this case Deep Learning, are not necessarily the most useful ones and its actually better to really understand what are all the possibilities each option bring us and evaluate which makes more sense for each case depending on the available data and expected outputs.\nSAR is something I havent heard before coming to UCL and its cool to be learning more and more each week, specially coming from such experts that make this complex topics seem easy to understand. Also after trying the practical I found it really interesting to replicate all the knowledge that Ollie has created for assessing this kind of issues. I keep wondering how else could this be used. I have previously analysed the changes of rivers over time and I found that they have stationality where they get fuller and emptier and it would be interesting to analyse it with EO using SAR but as far as i understood I would need to use the Phase for that, which is not available on GEE.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>An Introduction to Remote Sensing</span>"
    ]
  },
  {
    "objectID": "W6_GEE.html#summary",
    "href": "W6_GEE.html#summary",
    "title": "6  Google Earth Engine",
    "section": "",
    "text": "GEE Elements\nMeaning\n\n\n\n\nImage\nRaster\n\n\nFeature\nVector\n\n\nImageCollection\nA stack of images\n\n\nFeatureCollection\nA stack of features\n\n\nClient side\nFront end: The part that users interact with\n\n\nServer side\nBack end: Where the data is retrieved and processed\n\n\n\n\n\n\nScale & Resolution: GEE aggregates data based on zoom level—so the further you zoom out, the bigger each pixel represents. Scale and resolution are always linked!\nProjection: Everything is converted to the Mercator projection by default.\nFiltering: To avoid loading excessive data, we always filter by time and spatial bounds.\nOperations & Applications: GEE lets us perform geometric operations, corrections, enhancements, and even advanced tasks like machine learning, classification, and deep learning.\n\n\n6.1.1 Practical\nIn the practical, we applied these concepts to the city of Delhi, but I also decided to analyze a Peruvian city following the same steps. It was interesting to apply the techniques in a familiar context, though I encountered limitations, such as needing to increase the cloud presence threshold to obtain enough images. But overall it was amazing to finally use GEE and see how fast and practical it was to load the data for anywhere in the world and run the analysis.\nImage 1: PCA analysis of Iquitos, Peru",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Google Earth Engine</span>"
    ]
  },
  {
    "objectID": "W3_RemoteSensingData.html#summary",
    "href": "W3_RemoteSensingData.html#summary",
    "title": "4  Remote Sensing Data",
    "section": "",
    "text": "4.1.0.1 Key Terms:\n\nIrradiance: Radiation reaching Earth from the Sun\nRadiance: Radiation leaving Earth toward the satellite\nReflectance: The proportion of incoming radiation reflected by a surface\nSurface Reflectance: Reflectance at the bottom of the atmosphere (after atmospheric correction).\nAnalysis-ready data: Preprocessed images, usually in Surface Reflectance format (so we don’t have to do all the corrections ourselves!)\n\n\n\n\n4.1.0.2 Corrections:\n\nGeometric correction: Aligns misaligned images using control points and regression (basically shifting everything so it matches properly). Useful for correcting historical maps to match modern spatial data.\nAtmospheric correction: Removes distortions caused by atmospheric scattering (because the atmosphere its always there!).\nMethods include:\n\nDark Object Subtraction (DOS): Finds something that should have zero reflectance and subtracts its measured value from all pixels\nPseudo-Invariant Features (PIFs): Uses objects with stable reflectance over time as references for correction (great for long-term studies)\nFlat correction: Uses field-measured values (without atmospheric interference) as a baseline for regression\n\nOrthorectification: Fixes distortions from sensor tilt\nRadiometric calibration: Converts Digital Number (DN) to spectral radiance to standardize measurements\n\n\n\n4.1.0.3 Enhancements:\n\nContrast adjustments: Modifies image appearance without changing data values (basically, just making it look better)\nBand ratios: Dividing one band by another (e.g., NDVI for vegetation analysis)\nFiltering: Can be low-pass (smoothing) or high-pass (highlighting edges, like for detecting buildings)\nPrincipal Component Analysis (PCA): Reduces dimensionality to capture the most variance (helps focus on key changes in images, but could loose some interpretability)\nTexture analysis: Measures similarity between a pixel and its neighbors (useful for spotting urban areas or specific land features)\nImage fusion & pan-sharpening: Merges data from different sensors, often using high-resolution bands to sharpen lower-resolution images",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Remote Sensing Data</span>"
    ]
  },
  {
    "objectID": "W7_Classification.html#summary",
    "href": "W7_Classification.html#summary",
    "title": "7  Classification",
    "section": "",
    "text": "7.1.1 Some foundational concepts:\nHuman learning: uses inductive learning, so given our experience of the world, we make inferences on the images or data we see\nExpert system: system that uses human knowledge as a base to solve problems. Tries to show a computer how humans reach decisions\nMachine learning: science of computer modeling of learning process trying to replicate inductive learning\nTwo schools: On one side, Traditional classifiers, they don’t apply a model, they just divide the data based on a feature space. On the other side, the newer methods use Machine Learning.\n\n\n7.1.2 Classification methods:\nLinear Regression: Predicts continuous values by finding the best fit between independent and dependent variables.\nDecision Trees (CART): Useful when linear regression assumptions don’t hold. Classification trees assign discrete categories, while regression trees predict continuous values by recursively splitting the data.\nRandom Forests: Multiple decision trees working together, increasing accuracy but reducing interpretability. The model votes on the most likely classification, which makes it more reliable but harder to understand.\nClustering/K-means: Unsupervised method, similar to DBScan, it makes clusters depending on similarities with other pixels. Useful when categories are unknown\nISODATA: Extension of K-means. Similar but has some inputs called hyperparameters\nMaximum likelihood, density slicing, parallelpiped : They are more classical methods not used that much nowadays. They are supervised methods that start with a class definition, selects the training data, and applies the model to the rest of the data.\nSupport Vector Machine SVM: Comparable to linear regression. Instead of a line there’s a hyperplane and many support vectors . This hyperplane separated two classes but allows some misclassificationndefined(soft margin). It’s a two class comparison but could be replicated for multiple classes.\nNeural Networks & Deep Learning: We didn’t cover these in detail, but they represent the next level, more powerful but often a “black box,” meaning high accuracy at the cost of interpretability.\n\n\n7.1.3 Machine learning concepts:\nOverfitting: When the model fits the training data too well that makes it useless for new data. In order to avoid it we have to check for low bias (difference between predicted value and true value) and low variance (difference in accuracy between train fit and test fit)\n\nTrain and test data: we develop the model with train data and it doesnt see the test data. Then I use that model to predict other pixels including validating(test) pixels. With this comparison i can assess the accuracy(which we will learn more next week)\nCross-validation: same process but changing what is your train and test data each iteration. It ensures a more robust model.\nBootstrap sampling: sampling by replacement. it means some rows of data can be duplicated\nOut of bag sample: rows of data not used for the random forest. It is used like a validation dataset and is useful to get the out of bag error\nSupervised classification: The model learns from labeled training data to classify new data.\nUnsupervised classification: The model classifies data given a method or algorithm. It has no human input and categories are not known a priori\nHyperparameters: control variables\nBlack box: you just know if the model is good but loose interpretability. The most accurate models usually have less interpretability.\nSpatial autocorrelation: if the train and test data are too close spatially, they are probably very similar and the accuracy could be too high but the model not robust enough (overfitting)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "W9_SAR.html#summary",
    "href": "W9_SAR.html#summary",
    "title": "9  Synthetic Aperture Radar",
    "section": "",
    "text": "VV (Vertical-Vertical): Sensitive to rough surfaces and water bodies. *\nVH (Vertical-Horizontal): Detects volume scattering from vegetation. *\nHH (Horizontal-Horizontal): Picks up double-bounce reflections, often seen in urban areas.\n\n\n\nRough surface scattering – Common in open terrain, chaotic reflections. Very visible in VV polarization\nVolume scattering – Occurs in vegetation or complex surfaces. Mostly sensitive in VH polarization\nDouble bounce scattering – Happens with vertical structures like buildings. Mostly sensitive in HH polarization\n\n\n\n\nAmplitude (Backscatter): Measures the “loudness” of the returned signal, useful for texture analysis.\n\n\n\nPhase: Measures when is the reflected signal received. It allows us to measure distance to the ground and change detection over time, but it’s not available in Google Earth Engine (GEE).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>An Introduction to Remote Sensing</span>"
    ]
  }
]